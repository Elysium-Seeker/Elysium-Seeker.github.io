[{"content":"被研讨课老师赏识于是派发任务了（？）\n写篇博客记录下阅读心得吧，后面做综述也方便点。\n说是看了不少实际上完全没有阅读全文的能力，都只能对着AI一点点学核心内容。。但应该算大致理解他在干什么了。\n[VAE] Auto-Encoding Variational Bayes (2013) 首先是万物之源 $VAE$。\n$VAE$ ，全称 $Variational\\ Autoencoder$ ，中文叫变分自编码器。\n主要解决的问题是在数据量巨大且潜变量的后验分布复杂不可解的情况下，用正态分布逼近的方法得到一个变分下界，并在对变分下界重参数化后使其可通过标准梯度变化方法求解。\n具体而言，文章搭建了 $Encoder——Decoder$ 的编码/解码器架构双神经网络架构。$Encoder$ 主要负责提取图片特征并得到对应潜变量分布，$Decoder$ 则负责在 $Encoder$ 得到的潜变量分布中采样并转成图片。\n此方法面对的主要问题有：\n潜变量的原始后验分布复杂且不可计算。\n$Encoder$ 得到的是潜变量分布的均值和方差，而传统 $Decoder$ 的方式是在对应范围内随机采样，这会导致神经网络反向传播训练更新参数的工作无法进行（没有有意义的梯度）。\n而文章采用的解决办法是：\n设计了新的 $Loss\\ Function$ : $$ Total\\ Loss = Reconstrction\\ Loss + KL\\ Divergence$$ $Reconstrction\\ Loss$ （ 重构误差 ）依据生成图片与原图的像素差计算，目的是保证两者的相似程度。而 $KL\\ Divergence$ （ KL散度 ）则解决了上面的第一个问题，它衡量的是 $Encoder$ 预测的潜变量分布与标准正态分布 $N(0,1)$ 的相似程度。从而控制了潜变量的分布，强制的把它拉到了一个近似的标准正态分布上。\n另外的，数学上证明了： $$log\\ P(X)=ELBO+KL(后验||近似)$$ $log\\ P(X)$是数据出现的概率，我们的目的是使其尽量大，而此处的 $KL$ 散度一定是正的，所以只要让 $ELBO$ （ 变分下界，即上面的 $Loss Function$ 的相反数 ）越大就能让 $log\\ P(X)$ 尽量大\n发明了一种新的采样方法：重参数化。通过引入一个独立的，不依赖于任何参数的标准噪音 $\\epsilon$ ，并将采样方法从潜变量分布内随机抽样改为采 $$z=\\mu+\\epsilon \\times \\sigma$$此时 $z$ 由随机值变为带随机常数项的带参确定性算式。且注意到在统计学意义上标准噪音的均值为 $0$，故此时在保证鲁棒性和潜变量分布连贯性的同时使梯度变得有意义，可用于训练时的反向传播。\n于是最后得到了一组可解码图片并将噪音生成对应图片的模型，也是图像生成领域的基石。\n[GAN] Generative Adversarial Nets (2014) 接下来是 $GAN$\n$GAN$，全称 $Generative\\ Adversarial\\ Nets$ ，生成对抗网络。\n本质是一种酷炫的思想，相对来讲少一些数学。\n与 $VAE$ 不同，这次我们设计的两个神经网络从相互合作变为了相互对立的关系。\n生成器 $Generator$ （ 后简称 $G$ ）负责用随机的噪音生成图片，而判别器 $Discriminator$ （ 后简称 $D$ ）负责判断给到的图片是真图（ 即来自数据集 ）还是假图（ 即由 $G$ 生成 ）。\n而在训练过程中，依靠梯度反向传播， $G$ 的生图能力和 $D$ 的判别能力将同步提高，在理想情况下最后将达到纳什均衡，即 $G$ 生成的图与真图的分布完全相同，此时 $D$ 只能对所有输入的图都以各二分之一的概率判断是真/假图（否则都有可能降低判断的准确率）且 $D$ 和 $G$ 均不再改变。\n此时的论文中，$G$ 使用的是多层感知机 $MLP$ ( 在后续将被优化成反卷积网络 $CNN$ )，而 $D$ 是个单纯的二分类神经网络。\n尽管 $D$ 只能输出对于图片真假的判断，但是内部参数的梯度（ $Loss$ 对每个权重的偏导数 ）在反向传播的过程中可以通过调整权重矩阵/卷积核使 $G$ 明白假图和真图的差异（ 即怎么改图可以让 $D$ 认为这个图更像真图 ），也使 $D$ 明白怎么更好的判断图是真是假（ 此时一部分卷积核对真图的特征有强烈的正向激活，另一部分卷积核对假图的特征有强烈的负向激活。）\n（其实卷积这一块我还是没完全搞懂，准确来说是神经网络的训练我不是很清晰，后面可以加点补习内容然后更新一下。）\n然后说一下这篇论文最为酷炫的 $minimax$ 公式，就是在训练神经网络中要使其最优的 $Value\\ Function $：\n$$\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))]$$对 $D$ 来说，目标是让 $\\mathbb{E}{x \\sim p{data}(x)}[\\log D(x)] $ 尽量大，让 $ \\mathbb{E}_{z \\sim p_z(z)}[\\log(D(G(z)))]$ 尽量小。也就是对真假的判断尽量准确，而对 $G$ 来说则正相反，二者动态平衡，从而在达到纳什均衡时得到理想的训练效果。\n当然实际上反复横跳的概率远比达到均衡要高。。。所以实际运用上效果没有理想情况那么好，但是这个思路后面发展出的 $DCGAN$ 以及 $StyleGAN$ 系列就能进行很好的工业运用了。\n[DCGAN] Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks 然后是把 $GAN$ 从理论转向工程的 $DCGAN$ 。\n$Deep\\ Convolutional\\ Generative\\ Adversarial\\ Networks$，深度卷积生成对抗网络。\n原本的 $GAN$ ，正如上文所受，使用的是 $MLP$ ，效果一般，训练不稳定且出图模糊。\n而 $DCGAN$ 则选择用 $CNN$ 代替 $GAN$，并给出了一套有效可复现可扩展的架构框架，具体而言：\n使用带步长的卷积进行 $CNN$ 的训练（原来是池化的），尽可能保证图片不丢失关键细节且修炼效果更好。\n在 $G$ 和 $D$ 的神经网络大部分层中采用批量归一化（即为避免网络层数过深导致的梯度爆炸或消失问题将参数分布强行拉到标准正态分布的级别，并允许模型自身根据需要微调），极大的稳定了训练效果。\n移除网络顶部的全连接层，让整个网络几乎全部由卷积层构成（当然 $D$ 的最后一步判别还得是 $MLP$ ），从而有效增强了模型的“空间感知”能力。\n给 $G$ 和 $D$ 选取了合适的激活函数（神经网络中的必要部分，在每次线性变换后对参数进行非线性扭曲从而提高模型普适性）。在 $G$ 中输出层选取的是 $Tanh$，生成层选取的是 $Relu$ 。而 $D$ 中选择的是 $LeakyRelu$。\n简单说下这几个激活函数是什么：\n（1） $Tanh$ 是双曲正切函数，数学定义是 $$Tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$$能把输入值归一化到 $[-1,1]$ 之间，且具有对称性。具体的图像如下：\n（2） $Relu$ 的公式是\n$$f(x)=max(x,0)$$可以有效的缓解梯度消失，是 $CNN$ 的标配，但可能会导致 $Relu\\ Dying$ 的问题：当一个神经元一直输出负数的时候，它就再也学不会了。\n(3) $LeakyRelu$ 和 $Relu$ 十分相似，公式为\n$$f(x)=\\begin{cases}x\\quad if\\ x\\ge0 \\\\ \\alpha \\cdot x\\quad if\\ x\u003c0\\end{cases}$$ $\\alpha$ 是非常小的常数（ 一定小于1 ）。这种方法有效赋予了梯度在负数区域的流动性，这对需要持续提供梯度以优化 $G$ 和 $D$ 效果的 $D$ 十分必要。\n而在 $GAN$ 得到这些增强变身为 $DCGAN$ 后，文章发现训练后的 $D$ 在无监督特征学习上表现出色，且第一次在视觉上展示了 $z$ 空间（ 潜变量空间 ）的语义。实验得到：\n$$z_{man\\ with\\ glasses}-z_{man\\ without\\ glasses}+z_{woman\\ without\\ glasses}=z_{woman\\ with\\ glasses}$$这体现了 $GAN$ 的潜变量空间是连续的，结构化的，可被计算，理解与操作的。同时通过可视化分析 $D$ 的卷积核和 $G$ 的记忆，发现 $D$ 在第一层卷积核就学会了检测非常基础的边缘和角落，这证明了 $D$ 确实在学习有意义的底层视觉模式；同时 $G$ 生成的图片往往是训练集中多个样本的平滑混合与重组，这证明了 $G$ 具有泛化能力，它通过解构和重组学会了创造，而不是简单的复制。\n在文章的最后，作者为今后音视频方向的 $GAN$ 发展和潜空间的进一步研究指明了方向，音视频方向暂且按下不表，潜空间则涉及到下一篇论文：\n[StyleGAN] A Style-Based Generator Architecture for Generative Adversarial Networks $StyleGAN$ 重新定义了高质量图像生存的标准，并且至今仍然是许多后续研究的基石（包括待会讲的 $StyleGAN2$ 和 $StyleGAN3$ 。\n原本的 $GAN$ 系列模型虽然已经能应用于生成较清晰的图片，但普遍存在一个巨大的问题：**潜变量空间 $z$ 是纠缠的$。换言之，由于我们把所有数据集拉到了一个标准正态分布里，它们的排列方式和疏密程度很不优雅，从而导致一个参数控制了多个相互独立的视觉特征。\n于是 $StyleGAN$ 重置了原本的 $G$ ，引入了三大核心组件来解决这个问题：\n**映射网络（Mapping Network）：**这是一个独立的 $MLP$ 网络，它不再让原始的噪音向量 $z$ 直接进入生成网络，而是先把它变换成用于解纠缠的潜变量 $w$。这样操作的好处在于我们不再强制地把所有的数据集挤到一个标准正态分布里，而是可以以更适配原来的数据分布的形式表示训练集，从而使得纠缠的 $z$ 变成了更容易线性分离的 $w$。（此后所有工作均在 $w$ 空间上进行，这个空间和真实图像空间相通。）为了让 $w$ 空间更加优美，避免突变， $StyleGAN$ 额外引入了 “感知路径长度” 正则化（ Perceptual Path Length Regularization ，PPL ），使得引入的 $w$ 空间更为平滑，不再有突变。\n自适应实例归一化（AdaIN）：\n$$ \\text{AdaIN}(x, w) = \\sigma(w) \\left( \\frac{x - \\mu(x)}{\\sigma(x)} \\right) + \\beta(w)$$ 公式如上，具体可以分为两个步骤，第一步为： $$( \\frac{x - \\mu(x)}{\\sigma(x)} )$$ 对这张特征图进行实例归一化，保留空间结构信息但是删去风格，第二步是：\n$$\\sigma(w) \\left( ... \\right) + \\beta(w)$$ 赋予其由当前层 $w$ 指定的全新风格。（这个操作并非只进行一次，而是分层（见下面第三点）进行，每层进行两次 “卷积 + AdaIN” 的操作。）\n分层控制（Hierarchical Control）：生成器从小分辨率逐渐放大到大分辨率，不同分辨率层控制的主要特征不同，且在 $AdaIN$ 的帮助下每层都可以独立控制。\n$StyleGAN$ 的核心思想是分离。它把纠缠的潜变量 $z$ 分离成了更纯净的 $w$ ，然后又把 $w$ 蕴含的特征分层注入到生成过程中的每一层，从而实现了对视觉特征的解耦合和精细控制。\n额外的，它固定了生图的起点，原来生成器的起点是随机噪音，而现在是一张固定的低分辨率的图，从而使得潜变量只负责风格，不需负责初始空间布局，进一步加强了模型的解纠缠能力。\n[StyleGAN2] Analyzing and Improving the Image Quality of StyleGAN $StyleGAN$ 的升级版，主要在修 Bug 。\n解决的两个主要问题是：水滴伪影和相位伪影。\n首先讲水滴伪影，图示是这样的： 可以明显观察到有一颗巨大的深色水滴状伪影在图上。\n它出现的原因是 $StyleGAN$ 最标志性的 $AdaIN$ \u0026hellip;\n$AdaIN$ 的第一步是归一化，在抹去风格的同时还破坏了特征图之间的相对强度信息，生成器为了保留某些重要的强信号，在训练中学会了在无关紧要的地方放一个极强的信号，来保留原来的弱小的有用信号们。\n改进方法是放弃归一化，转而直接根据风格（即进来的 $w$ ）调整卷积核。\n然后是相位伪影： 可以观察到这三张图片里的人物脸转动了，但是牙齿什么的都没动。\n它出现的原因是 $DCGAN$ 中我们采用的分层控制，或者说来自 $ProGAN$ 的 **“ 渐进式增长 ” (Progressive Growing) **。\n这种方法先练低分辨率 （ 4 * 4 ）， 再逐渐往高分辨率训练，每一层都倾向于过早锁定特征。比如低分辨率时模型为了最小化 $Loss Funtion$ 强行把牙齿这种高频细节画出来，在高分辨率的时候就没法改了。\n改进方法是采取新的类似 ResNet（残差网络） 的架构，使用 Skip Connections（跳跃连接）。由每一层练好再练下一层改为输出时把每一层上采样叠加。（换言之，就是原来是一层层训练最后只用最大分辨率的模型，现在是直接每层分开训练不同频特征并在最后把每层图片叠加起来。）\n除了这两个问题之外还有一些别的优化：\n惰性正则化 (Lazy Regularization) : 这是针对 $StyleGAN$ 里的 PPL 的优化， 作者发现没有必要每一步都算正则化，容易爆显存，每16步算一次就很不错。大大提升了训练效率。\n路径长度正则化 (Path Length Regularization) 的增强: 作者进一步强调了 $PPL$ 的重要性。并提出了能通过 $PPL$ 判断图像好坏。（ $PPL$ 低的图通常质量高）而且 $PPL$ 还让生成器的逆向操作变得十分容易，把真实照片映射回潜变量空间边的非常可靠。（从而成了 P 图神器）\n[StyleGAN3] Alias-Free Generative Adversarial Networks 目前为止的最新 $StyleGAN$ 了。（后面全转向 $Diffusion$ 了）\n主要解决了纹理粘连的问题：\n即当尝试用 $StyleGAN2$ 生成动画时，图片中大的部分（如人脸，五官）的变动没有问题，但是皮肤上的毛孔，发丝等细节没有跟着大的部分一起动。\n而 $StyleGAN3$ 能彻底消灭这种粘连，让 $GAN$ 生成的图像具有 平移/旋转等变性(Equivariance)。\n纹理粘连的原因是整个图像生成体系习以为常的像素网格，和不完美的数学操作。\n在深度学习里，我们默认图像是由一个个离散的像素点组成的矩阵。\n但在物理世界里，光信号是连续的波。\n所以我们的操作会导致混叠现象的出现。\n激活函数（比如前文提到过的 $Relu$ ）或上/下采样操作由于产生了许多数值上的突变，会产生非常多高频的信息。\n而我们的像素网格数是固定的，所以采样数是固定的，根据 香农采样定理 ，如果信号变化的频率太快， 超过了像素网格采样频率的二分之一，信号就没法正确记录。\n于是这些高频信号就被伪装成低频信号出现在图像里，产生混叠，从而导致了粘连的发生。\n作者的解决方法是几乎直接推翻了 $StyleGAN2$ 的全部底层逻辑。\n$StyleGAN3$ 的核心思想是：我们要假装神经网络处理的不是“离散的像素”，而是“连续的信号”。\n具体的，有如下三个改动：\n去除“坐标暗示” ：神经网络一旦得到关于“绝对坐标”的暗示就会把纹理固定上去，所以作者废除了 $Padding$ （边缘填充），从而使神经网络不知道边缘位置。作者还废除了 $Noise Injection$（随机噪声图）（前面好像忘记讲了，这个是指 $StyleGAN$ 里专门引入了一个随机噪声用来生成头发丝什么的细节，从而使成图更真实，但它是直接加在像素上的，会直接导致纹理固定。）\n换“理想低通滤波器”：引入 Sinc 滤波器，在每一层卷积，上采样，激活函数后都过一遍低通滤波器，把高频信号全部切掉。\n重新设计层级结构 把所有建立在离散基础上的操作写成了连续域上的数学公式，再近似到离散网络上。\n于是改动后得到一个拥有丝滑的 “等变性”（Equivariance） 的模型，解决了纹理粘连而且可以随便平移，内部世界模型效果更强的模型。\n除此之外，$StyleGAN3$ 还提供了一些数据增强的具体实现方法，比如大量平移旋转训练集什么的，可以更好的做到等变性。\n[SinGAN] SinGAN: Learning a Generative Model from a Single Natural Image $ICCV 2019 Best Paper Award$ ，$GAN$ 路子上的邪修。\n主要实现的问题是如何用小样本（比如一张图）生成类似的图片。\n它只用一张图片作为训练数据，学会这张图片内部的 “补丁分布”（Internal Patch Distribution） ，并借此可生成无线多张相似但布局不同的新图片。\n具体的实现上，它运用了 “分形”和“局部性” 的思想，用一个11*11的小窗口在图片中采样，从而得到海量的训练数据。\n$SinGAN$ 不学“宏观语义”，它学“微观纹理”和“宏观布局”。为了同时学到宏观和微观，它还设计了一个多尺度的分层形式，训练了多个分辨率的 $GAN$ ，从低分辨率开始训练，并以低分辨率的成图上放大后作为输入放到更高分辨率的 $GAN$ 中训练，利用残差学习（类似前面提到的残差网络）逐层添加特征。\n而它的判别器也比较特殊，判别器不看整张图，而是看图中的一个个小方块，判断这些子图是否在原图中出现过，从而保证了生成的图片在细节上真实，而整体布局不与原图完全一样。\n除此之外，为了保证生成的稳定性，作者还额外强制加了一个重建损失 (Reconstruction Loss)，确保在以某组特定输入下模型能生成原图。\n[DDPM] Denoising Diffusion Probabilistic Models 终于到 $diffusion$ 了。。\n从这里开始就是生图的另一个路子了。\n$DDPM$ 的核心思想在与，它定义了两个过程，都是 $T$ 步：\n前向过程 (加噪): $q(x_t|x_{t-1})$\n这是一个固定的过程，没有任何参数需要学习。 每一次，都在上一时刻的图片上，叠加一点点高斯噪音。 到了 $x_{T}$，图片就彻底变成了标准正态分布的噪音 $N(0, I)$。 反向过程 (去噪): $p_{\\theta}(x_{t-1}|x_t)$\n这是我们要训练的神经网络。 它的任务是：给定一张稍微有点噪的图 $x_t$，预测出它的上一时刻 $x_{t-1}$ 长什么样。 也就是：学会“去噪”。 而通过 $DDPM$ ，我们让模型学会预测我们一开始叠加的高斯噪音。\n这里的 $Loss Function$ 也相当简单，只是一个均方误差(MSE)：\n$$ \\text{Loss} = || \\epsilon_{\\text{真实}} - \\epsilon_{\\text{预测}}(x_t, t) ||^2$$而我们的训练过程从原来的生图判别演变成了：\n从数据集拿一张真图 $x_0$。\n随机选一个步数 $t$（比如第 500 步）。\n随机生成一个噪音 $\\epsilon$。\n根据公式，把噪音加到真图上，得到 $x_t$（这一步有闭式解，不用循环 500 次）。\n把 $x_t$ 扔给 U-Net 模型，让它猜：“刚才加的那个 $\\epsilon$ 是多少？”\n算 MSE，反向传播。\n接下来补充一些细节：\n训练使用的网络是 $U-Net$ ，因为输入输出都是图片且尺寸相同，$U-Net$ 非常适配。\n当前时间通过 $embedding$ 转化成向量传入网络。\n（论文核心内容）关于一步实现所有前向过程的方法和推导： 明白了！你是想要图片中提到的 “通过数学迭代（把 $x_{t-1}$ 展开，再把 $x_{t-2}$ 展开\u0026hellip;）” 这一步的具体数学推导过程。\n图片中省略了中间的计算步骤，直接给出了结论。实际上这利用了 高斯分布的可加性（重参数化技巧）。\n以下是完整的推导过程：\n根据 DDPM 的定义，每一步的前向加噪公式为： $$x_t = \\sqrt{\\alpha_t} x_{t-1} + \\sqrt{1 - \\alpha_t} \\epsilon_{t-1}$$ 其中 $\\epsilon_{t-1} \\sim \\mathcal{N}(0, I)$ 是当步加入的随机高斯噪音。\n我们的目标是推导出 $x_t$ 和 $x_0$ 的直接关系。\n第一步：展开 $x_{t-1}$\n我们知道 $x_{t-1}$ 是由 $x_{t-2}$ 生成的： $$ x_{t-1} = \\sqrt{\\alpha_{t-1}} x_{t-2} + \\sqrt{1 - \\alpha_{t-1}} \\epsilon_{t-2} $$将这个式子代入到 $x_t$ 的公式中：\n$$ \\begin{aligned} x_t \u0026= \\sqrt{\\alpha_t} (\\underbrace{\\sqrt{\\alpha_{t-1}} x_{t-2} + \\sqrt{1 - \\alpha_{t-1}} \\epsilon_{t-2}}_{x_{t-1}}) + \\sqrt{1 - \\alpha_t} \\epsilon_{t-1} \\\\ \u0026= \\sqrt{\\alpha_t \\alpha_{t-1}} x_{t-2} + \\sqrt{\\alpha_t (1 - \\alpha_{t-1})} \\epsilon_{t-2} + \\sqrt{1 - \\alpha_t} \\epsilon_{t-1} \\end{aligned} $$第二步：合并噪音 (关键步骤)\n现在式子后面有两项噪音：\n$\\sqrt{\\alpha_t (1 - \\alpha_{t-1})} \\epsilon_{t-2}$ $\\sqrt{1 - \\alpha_t} \\epsilon_{t-1}$ 根据高斯分布的性质：两个独立的高斯分布相加，依然是高斯分布，且方差相加。 即：若 $X \\sim \\mathcal{N}(0, \\sigma_1^2)$，$Y \\sim \\mathcal{N}(0, \\sigma_2^2)$，则 $X+Y \\sim \\mathcal{N}(0, \\sigma_1^2 + \\sigma_2^2)$。\n我们计算这两项噪音合并后的总方差：\n$$\\begin{aligned} \\text{总方差} \u0026= (\\sqrt{\\alpha_t (1 - \\alpha_{t-1})})^2 + (\\sqrt{1 - \\alpha_t})^2 \\\\ \u0026= \\alpha_t (1 - \\alpha_{t-1}) + (1 - \\alpha_t) \\\\ \u0026= \\alpha_t - \\alpha_t \\alpha_{t-1} + 1 - \\alpha_t \\\\ \u0026= 1 - \\alpha_t \\alpha_{t-1} \\end{aligned}$$所以，这两项噪音可以合并为一个新的噪音项： $$\\text{合并噪音} = \\sqrt{1 - \\alpha_t \\alpha_{t-1}} \\bar{\\epsilon} \\quad (\\text{其中 } \\bar{\\epsilon} \\sim \\mathcal{N}(0, I))$$此时公式变为： $$x_t = \\sqrt{\\alpha_t \\alpha_{t-1}} x_{t-2} + \\sqrt{1 - \\alpha_t \\alpha_{t-1}} \\bar{\\epsilon}$$第三步：继续展开 (递归)\n如果我们继续展开 $x_{t-2}$，以此类推\u0026hellip;\n我们会发现规律：\n$x_0$ 前面的系数是所有 $\\alpha$ 的乘积的平方根。 噪音前面的系数始终是 $\\sqrt{1 - (\\text{所有 } \\alpha \\text{ 的乘积})}$。 定义累乘系数 $\\bar{\\alpha}t = \\prod{i=1}^t \\alpha_i = \\alpha_t \\times \\alpha_{t-1} \\times \\dots \\times \\alpha_1$。\n经过 $t$ 次迭代展开后：\n$$\\begin{aligned} x_t \u0026= \\sqrt{\\alpha_t \\alpha_{t-1} \\dots \\alpha_1} x_0 + \\sqrt{1 - (\\alpha_t \\alpha_{t-1} \\dots \\alpha_1)} \\epsilon \\\\ \u0026= \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon \\end{aligned}$$这就是为什么我们不需要写一个 for 循环迭代 500 次，而是可以直接用闭式解（Closed-form solution）一步算出任意时刻 $t$ 的图像 $x_t$：\n$$x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon$$ 基于随机微分方程有结论：当加噪步长足够小时，逆过程也可近似于高斯分布，所以反向过程中只要预测上一张图高斯分布的均值和方差。实验发现方差固定即可，只要算均值就行。然后通过类似VAE的思想再进行一些数学操作就可以得到我们需要的 $Loss Function$。且容易由上面的推导发现算出噪声就可以算出原图。 [DDIM] Denoising Diffusion Implicit Models 一个对 DDPM 的优化。\n我之所以要花大篇幅讲上面的推导就是因为这里要用。\nDDPM的设计有个限制在于：为了保证逆向过程也是高斯分布，前向过程必须是马尔可夫链（一步一步加噪）。虽然我们在推导后实际上一步解决了前向，但逆向的过程还是得一步步来。\n而正由于我们用的是一步解决的式子 $$x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon$$ 所以只要 $x_t$ 符合这个分布，Loss就能算，U-Net就能练，中间的加噪过程不重要。\n于是 DDIM 用数学技巧编了个符合边缘分布的过程从而加速了训练，具体如下。\n在 DDPM 的反向采样公式里，有一项是随机噪音：\n$$ x_{t-1} = \\text{预测的均值} + \\sigma_t \\cdot z $$这个 $z$ 是随机的高斯噪音。正是因为有这个 $z$，每次生成的路径都乱七八糟。\nDDIM 作者推导出了一通用反向公式，其中包含一个可以调节的参数 $\\sigma$：\n当 $\\sigma = \\eta$ (特定值) 时： 即DDPM （随机游走）。 当 $\\sigma = 0$ 时： 随机项消失了，即 DDIM ，此时从 $x_t$ 到 $x_{t-1}$ 的过程变成了完全确定性 (Deterministic) 的。 虽然数学推导很复杂，但 DDIM 最终落地的采样公式非常直观。它把“计算 $x_{t-1}$”拆解成了三个逻辑步骤：\n假设我们现在在第 $t$ 步，手拿着 $x_t$，U-Net 预测出了噪音 $\\epsilon_\\theta$。\n第一步：预测“原本的真图” ($x_0$)\n已知 $x_t$，和大致的噪音 $\\epsilon_\\theta$，可预测 $x_0$ 为：\n$$ \\text{预测的 } x_0 = \\frac{x_t - \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon_\\theta}{\\sqrt{\\bar{\\alpha}_t}} $$第二步：指向“下一步的噪音”\n我们要去的下一站是 $x_{t-1}$ （或者跨步到 $x_{t-\\Delta}$）。 我们需要计算下一站应该保留多少噪音：\n$$ \\text{指向噪音的方向} = \\sqrt{1 - \\bar{\\alpha}_{t-1}} \\cdot \\epsilon_\\theta $$第三步：组合 (Vector Addition)\n把“预测的真图”和“指向噪音的方向”按比例拼起来：\n$$ x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}} \\cdot (\\text{预测的 } x_0) + (\\text{指向噪音的方向}) $$于是反向过程也可以加速了，且在训练足够完备（预测的噪音够准）的情况下也可以随便加速。\n最后同样的补充一些 DDIM 的特性\n语义插值 (Semantic Interpolation) 因为采样过程是确定的，一个噪音 $x_T$ 唯一对应一张图 $x_0$，所以可以取两个噪音 $z_1$ (生成狗) 和 $z_2$ (生成猫)，做球面线性插值 (Slerp)，中间的噪音生成的图，就是一只“像猫又像狗”的生物，DDPM 做不到这一点，因为随机性太大，插值出来的图通常是崩坏的。。\n图像重建/逆向 (Inversion) 这和 StyleGAN 的 Inversion 很像。可以把真图得到一个特定的噪音，并通过这个噪音还原图片。\n[Guided Diffusion] Diffusion Models Beat GANs on Image Synthesis 到此之前，Diffusion系的模型还是只能做到随机抽卡。（其实 GAN 那边也没有具体讲怎么对应标签，但是可以实现潜空间变量算数的话怎么做标签对应和指定内容生成应该还算显然。）\n那么如何引导去噪过程，让它朝着我们想要的方向去噪生成呢？\nOpenAI 提出了 Classifier Guidance（分类器引导）的方案，具体的：\n准备工作：\n训练一个图像分类器 (Classifier)，比如 ResNet。 关键点： 这个分类器必须在带噪的图片上训练过（能看懂充满雪花点的图）。 生成过程：\nU-Net 说：“我觉得这一步应该往左走 （去噪）。” 分类器介入： 它看一眼当前的图，计算一下梯度的方向——“往哪个方向改像素，能让这张图更像‘狗’？” 合力： 最终的移动方向 = U-Net 的去噪方向 + 分类器的梯度方向。 数学原理：为什么要加分类器的梯度？\n我们的目标是生成符合标签 $y$ 的图像 $x_t$，即从条件概率 $p(x_t|y)$ 中采样。 根据贝叶斯公式 $\\log p(x|y) = \\log p(x) + \\log p(y|x) + C$，两边同时对 $x_t$ 求梯度：\n$$ \\underbrace{\\nabla_{x_t} \\log p(x_t|y)}_{\\text{我们需要的目标方向}} = \\underbrace{\\nabla_{x_t} \\log p(x_t)}_{\\text{U-Net 的去噪方向}} + \\underbrace{\\nabla_{x_t} \\log p(y|x_t)}_{\\text{分类器的梯度方向}} $$公式详解：\n$\\nabla_{x_t} \\log p(x_t)$ (U-Net 的去噪方向)：\n这是原始 Diffusion 模型学到的东西。 它的作用是：“不管是什么物体，先让这张噪点图变得像一张‘真实的图片’再说。” $\\nabla_{x_t} \\log p(y|x_t)$ (分类器的梯度方向)：\n这是分类器提供的指引。 它的作用是：“不管图真不真，先让像素朝着‘更像狗’（标签 $y$）的方向移动。” 相加 (合力)：\n通常我们会引入一个 Guidance Scale (引导刻度 $s$) 来控制分类器的权重。 最终公式：新方向 = 原方向 + $s \\times$ 分类器梯度。 这玩意儿的痛点在于分类器训练极其麻烦，而且同时跑两个模型很占显存，于是在谷歌的天才们几个月后提出 CFG 之后这玩意儿基本没什么用了，但它是后续几乎一切的思路基础。\n[CFG] Classifier-Free Diffusion Guidance 正如前文所说，CG 的弊端不少：训练麻烦，算力消耗大\u0026hellip;\n于是在此基础上有了 CFG 。\n具体的，它不再需要额外挂载和训练一个专门的分类器，而是在训练时有一定概率丢掉标签，从而让 U-Net 学会了有提示词生成和无提示词生成时的噪音。于是通过如下的采样公式得到最后应有的目标方向： $$ \\tilde{\\epsilon}_\\theta(x_t, c) = \\epsilon_\\theta(x_t, \\emptyset) + w \\cdot (\\epsilon_\\theta(x_t, c) - \\epsilon_\\theta(x_t, \\emptyset)) $$符号说明：\n$\\tilde{\\epsilon}_\\theta$：最终用于去噪的预测噪音。 $\\epsilon_\\theta(x_t, c)$：有条件预测（Conditional）。 $\\epsilon_\\theta(x_t, \\emptyset)$：无条件预测（Unconditional）。 $w$：引导刻度 (Guidance Scale)，控制模型对提示词的遵从程度。 补充一些 $w$ 的取值影响：\n$w = 1$：相当于没有使用 CFG 增强。模型就按标准的条件概率生成。 $w \u0026gt; 1$ (比如 7.0 - 10.0)：这是常用区间。 模型会严格遵守提示词。 图像的语义更清晰，但多样性会降低。 $w$ 过大 (比如 \u0026gt; 20)： “过拟合”现象：图像会出现颜色过饱和、怪异的伪影、线条崩坏。特征向量被拉得太长，超出了自然图像的流形分布（Manifold）。 [Stable Diffusion] High-Resolution Image Synthesis with Latent Diffusion Models 终于到 Stable Diffusion 了，这是目前的 Diffusion 模型集大成者。\n我们前面讲的以 DDPM 为基础的 Diffusion 模型，计算量十分恐怖，因为它每个像素都要跑，而且训练时还要迭代数百步，每一步都要算梯度和噪音。且最后的成图中把大量的算力浪费在了不重要的细节上，而对语义相关的重点却没有多加关注。\n为了解决这一问题，Stable Diffusion 采取的做法是：去一个压缩空间中进行扩散。\n作者发现图像生成可以拆成两步：\n感知压缩 (Perceptual Compression)： 负责处理像素级的细节（清晰度、纹理）。这部分不需要太精细的计算，清晰即可。 语义生成 (Semantic Generation)： 负责处理内容（构图、物体关系）。这部分对算力和理解的需求较高。 于是 SD 采取了先把图片压缩，让 Diffusion 在小图上工作，最后解压的办法。\n第一步和第三部都是传统 VAE，第一步通过 Encoder 把图片压缩并保留核心语义信息，第三步把抽象的 Latent 转成原图。\n第二步则是 Diffusion ， 但与常规的 Diffusion 不同 ， 我们复原的是 VAE 压缩出的 Latent 。（但具体过程也没什么差别）\n那么在处理好画图问题之后，剩下的就是语义理解问题了。\n为此， SD 引入了 Cross-Attention 机制：\n即语言模型（在原论文中是CLIP，专用的图像对齐模型，现在则是各家的先进LLM）将输入的文本转为一串向量，并将其注入 UNet 。UNet 则通过 Cross-Attention 机制实现在生成时同时接收 Latent 和输入的向量（与 CFG 相似的做法）从而生成指定图片。\n受不了了汇报内容先截止到这里了，感觉完全够讲20分钟了。\n后续要看的内容还有\nNeural ODEs (神经常微分方程)（把 ResNet 的离散变成连续，从而实现材料演化的相关功能） NCA (Neural Cellular Automata)（像生物细胞一样无指挥“长”出图片） VQGAN (Taming Transformers for High-Resolution Image Synthesis, 2021) （Stable Diffusion 的前置科技。） ControlNet（给 Diffusion 加“骨架”。） LoRA（微调神器） （快速让模型学会一种新的“材料质感”（比如生锈金属），只需几十张图。） ","date":"2025-12-03T09:00:00+08:00","permalink":"https://Elysium-Seeker.github.io/p/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","title":"图像生成论文阅读笔记"},{"content":"写在前言之前 哇断更好久好久了。\n果然周更什么的不太可能。\n最近也是忙的没时间写博客了呜呜呜。\n不过也算是攒了很多可以写的素材~\n后面会写一篇Cyberpunk2077的游记（等一回目之后），一篇蓝信封以及志愿者协会目前的故事，然后会把自家的小斯幽调的足够可爱之后写篇文章介绍的。\n那么先来介绍一些这次的产品吧。\n链接：https://github.com/Elysium-Seeker/Digital-Economy-Article-Aggregator\n前言 这次的灵感来源是数字经济通识课程的日常作业。\n要求我们每周查找相关领域的权威资料并分享。\n而老师特意表扬的一组里除了资料本身还有信源和摘要。\n再加上自己曾经订阅过一个做推上ai信息收集然后每日整理发到邮箱的有趣项目。\n所以就想着自己做一个类似的简单agent了。\n项目介绍 这是一个数字经济通识课程的资料查找作业工具。\n可用于查找一周内相关领域的权威文章/论文/报告。\n专注于国家级、国际化的数字经济研究，优先提供学术论文和深度行业报告，并确保中英文文献均衡呈现，为您提供全球化的深度洞察。\n可以限定关键词，关键词的英文同义词也将作为检索词，用于查找相关的外文文献。\n还会提供文章的摘要和信息源，帮助用户速览文章的大致内容与确保内容的权威性。\n开发过程 其实一开始没想着弄这个的。。。\n只是研讨课上想用 gemini 查点资料水掉作业而已。\n结果一时起兴点进了 google ai studio 的 build 页面，突然就有了做个 agent 的想法。\n一开始非常努力的在使用英文描述需求。\n其实效果不错，但太费精力了。（尤其是手机打英文真的痛苦）\n而且最终提交上去的摘要肯定还得是中文。\n于是就加了句 turn the website to Chinese。\n结果 build 用的 agent 直接开始用中文回我了。\n那也就用中文一步步改下去了。\n在纯用自然语言描述的情况下能做出这么精致的一个 agent 还是很体现 gemini 2.5 pro 的实力的。\n不过 bug 也确实是要人一步步催着它修的。\n英文改中文后一个冒号的事故改了好几轮 conversation。\n（事故来源是模型内部提取结构化信息的时候中英文冒号混用无法识别导致一篇文章都找不到。）\n以及现在还是时不时会找到无法访问的一些文章，信源也算是良莠不齐（在努力调prompt了但是效果不算特别理想）。\n但总之作为一个作业工具已经非常合格啦！\n后记 感觉发现了 ai 助力生活的新大陆。\n算是第一次创作一个自己用的到的 agent。\n开森，要再接再厉哦！\nUpdate：发现其实自己绕了一大圈工作流程，明明找个周更的稳定信源就可以了搞那么麻烦，属于是第一性原理发力了（，项目也已经删了w，期待由相同思路的后续吧。\n","date":"2025-10-22T09:00:00+08:00","image":"https://Elysium-Seeker.github.io/p/digital-economy-article-aggregator%E9%A3%9F%E7%94%A8%E6%8C%87%E5%8D%97/image_hu_b115ca571574de9f.png","permalink":"https://Elysium-Seeker.github.io/p/digital-economy-article-aggregator%E9%A3%9F%E7%94%A8%E6%8C%87%E5%8D%97/","title":"Digital Economy Article Aggregator食用指南"},{"content":"把之前夏促买的纪念碑谷本章打完了，被游戏的画面和错位美学深深震撼到，于是写个微评。\n游戏的主要流程是通过机关改变地图与视角，通过视觉错觉构造路径让公主艾达到达终点，期间会有乌鸦人之类的阻碍，也会有图腾之类的辅助机关。（其实乌鸦人也并不完全是阻碍，有时候也是触发机关不可或缺的助益。）\n总的来说是一款非常美丽的游戏，画面色彩很好看也很治愈。\n第一-五关 先美图镇楼（）\n前面几关主要是熟悉玩法什么的，包括乌鸦人和图腾这种NPC角色的引入，很有趣同时难度也不算特别大，没怎么卡关就一遍过了。\n以及最后一张（应该是第五关结尾）的新月真的好好看！\n第六-七关 从这一章开始有卡关了\u0026hellip;\n红海那一块移图腾的地方卡了好久才研究明白。\n这种小巧思也让这个游戏可以打上益智tag呢。\n第六关结尾时公主出海然后图腾在后面努力跟上的场面真的好宏大好美丽可惜没截屏到，有种史诗感和孤独感。\n然后第七关的配色我也很喜欢，妹妹说紫色很有韵味。\n第八关 本章中最让我感到震撼的关卡就是第八关。\n当我发现这个盒子从四个方向打开的内容不一样，但都如此和谐，彼此之间还有联动的时候，我整个人都被surprise到了，真的感觉好有趣好惊喜。\nPart 1 需要小乌鸦人单过一关的小设计也很有趣。\n而当 Part 1 的四部分走完之后， Part 2 的部分更为精彩。\n整个盒子先是展开成城堡一样的建筑，而到了最后，在解迷结束、公主到达终点时，建筑再一步步地关上、合拢，变回原来的盒子，就像这里贴的图一样。\n复原的画面不止震撼，而且莫名的治愈和解压。在我印象里，我从未见过如此和谐的融合了建筑美学、几何美学与机械美学的场景。\n第九关 与堪称“巧夺天工”的第八关相比，第九关的创意与美感也是不遑多让。\n首先在稍显常规的 Part 1 里有个捡花的伏笔，很可惜没有录屏到。\n然后就是恢弘且电影感十足的一路向下的地图。\n颜色从明黄逐渐变暗，到了最后是淡淡的灰色。在一步步下行的过程中，一种肃穆的感觉于我心头油然而生。\n而到了最后：\n公主在墓前献上了花，或许这就是艺术吧。\n第十关 本章的最后一关，也是集大成者，前面的元素与巧思基本都有参与。\n繁复精巧的错位卡了我不少时间，各种机关也是。\n最后使出了倒推法才算是顺利的解决了这关。\n不过相比于第十关我更喜欢前两关一点\n通关之后 很遗憾没有截上图。\n但所有乌鸦人被解放成五彩斑斓的小鸟，小公主也带上王冠成为它们中的首领，唯美的画面也称得上 \u0026ldquo;unforgettable\u0026quot;了。\n总而言之是一部非常有趣有新意的作品，其中的错位美学与各种巧思给人的震撼和美的体验即使在这款游戏发售十一年后也未减分毫。在我看来，这就是第九艺术。\n剩下的部分等附录和Ida\u0026rsquo;s dream打完再更，以及第二部我也买了，说不定也会更，至于3\u0026hellip;等打折吧阿巴阿巴。\n","date":"2025-07-31T16:37:50+08:00","image":"https://Elysium-Seeker.github.io/p/%E9%94%99%E4%BD%8D%E7%BE%8E%E5%AD%A6%E7%9A%84%E7%AC%AC%E4%B9%9D%E8%89%BA%E6%9C%AF/image_hu_202503c1c9deffa.png","permalink":"https://Elysium-Seeker.github.io/p/%E9%94%99%E4%BD%8D%E7%BE%8E%E5%AD%A6%E7%9A%84%E7%AC%AC%E4%B9%9D%E8%89%BA%E6%9C%AF/","title":"错位美学的第九艺术"},{"content":"周更是不可能周更的，尤其是期末周加社会实践这种阴间配置情况下。暑假也写不动，能更多少更多少吧。\n前段时间白嫖了上海创智学院的“走进大模型”暑期微课营，并在其中做了个简单的项目，于是写篇文章记录一下。\nDay 1-3 主要是上课，大模型基础知识什么的。\n对这地方的第一印象是好有钱，免费的营甚至包饭还发衣服（放外面不得四位甚至五位数），而且教室也好大好新。\n还非常幸运的发现有合唱团的学长准备在这里读研，并进行了一番深入的了解。\n第一天下午有这里几个重点项目的介绍，对情境智能很感兴趣。毕竟AI拟人化是我一直以来的目标。\nActually有点点来这里读研的想法，不过好像很卷很难进\u0026hellip;\nAnyway，有自所保底，也不是很慌，不急（确信）。\n从上课的内容里面算是大致了解到了大模型的部分原理以及微调什么的，有很多是之前知道的甚至高中技术学到的，当然也有很多自己上学期看相关内容了解过的部分，不过还是受益良多的，起码现在算是大致明白了之前一直很好奇的Attention is all you need的大致内容，也了解了模型训练的一个流程什么的。（虽然其实没怎么听，边摸鱼边靠Gemini补课。）\n感觉黑掉模型让它说脏话的部分最有意思了！（bushi）\n（以及上课的时候看旁边华东理工老哥玩进阶20的Slay the Spire 也很有意思。）\n趁着上课时间还摸鱼做了个动态世界模拟器，虽然也是很多bug没调完而且把自己的copilot用到限额了，但是确实很好玩，大模型结合数据库的思路感觉也是后面可以发展的方向。（好像现在Astrbot做的长期记忆就是这个方向搞的，用的milvus数据库）\nDay 4 算是最忙的一天了。\n和组里的同济队友合作做出了我们的AI-STOCK。\n虽然现在仍然BUG颇多，而且我暂时也用不了它了（dify临时账号被停了）\n从一开始的模拟人生到后来想到股票方面找到自己之前试用的ai-hedge-fund再到一步步优化调整成我们最后的产品以及画饼PPT和各种完善方向等等，都是非常有意思的过程。\n比预想中的搭建Agent有趣，所学到的知识也远超预期。\n以及上班必备的摸鱼和临时性加班也是让人哭笑不得。\n下午四点就做完\u0026amp;提交了所需要的所有材料然后全组摸了一个小时鱼，结果5点下班的时候突然通知说要做海报要留一会儿才能走，再过了半小时重新询问又发现可以线上做\u0026hellip;\n然后快7点才给到所有材料，又临时给了7点半的ddl。还好有靠谱的美工队友，卡着7点28分最终是发出了材料。\nAfter all，过的很值得铭记呢。\nDay 5 结营日\u0026amp;汇报日，主要流程就是看汇报\u0026amp;做汇报，以及最后的结营。\n很有趣的是发现有两个小组做了和我们相似的内容，不过全部了解过后我坚定的认为我们是做的最完善的。也是画饼画的最好看的。\n本来不想汇报的本人在队友的推举下不得不一个人搞了全部的汇报，所幸还算顺利没出什么岔子。\n以及从海报上能看出大学生真的都是班味十足，做的都是投资、面试之类的工具类Agent，反观高中生们的作品都是充满想象力的文字类游戏，什么凡人修仙传狼人杀都来了。（虽然相对没有技术含量但确实是大模型擅长的领域而且也确实很好玩呢）\n最后结营的时候又被经费震撼了一下，这边是真有钱免费的营不仅发证书还有纪念品。\n走之前和助教交流了一下这里怎么进，还是很感兴趣的，希望未来能有考到这里，做自己喜欢的项目的机会。\nAI-STOCK介绍 做都做了肯定要拿来水一水内容的啦！\n把yml文件传到github上了，网址在这里\n（不过readme是AI写的，画饼什么的都不太可能实现，事实是我现在因为手边没有GPT的API接口甚至无法调试它\u0026hellip;）\n具体来说，这是一个“基于多Agent系统的下一代投资决策支持平台”。\n本质在于让多个大模型从不同的角度给出投资建议并进行整合，以此达到全面和个性化的目的。\n在做投资分析的同时额外还补充了图表生成，信息聚合和新闻等几个模块，勉强算是一站式投资信息门户平台了。\n（其实我觉得很有商业价值啊，做到财经类APP里做个小插件应该很有前途的，就叫\u0026hellip;AI投资展望？）\n关于这个Agent的具体内容大概如下：\n✨ 核心亮点 (Key Features) 🤖 多Agent协同分析 (Multi-Agent Analysis): 系统由十多个拥有不同“人格”和分析框架的AI Agent组成，并行工作，确保分析的广度和深度。 🧠 传奇投资人视角 (Legendary Investor Personas): 借助沃伦·巴菲特的价值洞察、凯茜·伍德的创新远见、彼得·林奇的成长嗅觉，从经过时间检验的投资哲学中获得智慧。 📄 综合策略报告 (Comprehensive Reporting): 最终输出一份详尽的“投资委员会会议纪要”，清晰呈现每位专家的观点、分歧与共识，以及最终的综合策略建议。 💬 智能查询与交互 (Intelligent Query \u0026amp; Interaction): 不仅能进行深度分析，还能理解用户的日常查询，如“苹果公司最近有什么新闻？”或“给我看一下特斯拉近一年的股价图”。 🌐 动态数据聚合 (Dynamic Data Aggregation): 自动从网页、API等多种渠道获取最新的财务报表、新闻和市场数据，确保分析的时效性。 🏛️ 系统架构与工作流程 (Architecture \u0026amp; How It Works) AI-STOCK 的工作流程旨在模拟一个高效的投研团队从接收任务到提交报告的全过程。\n用户输入 (User Query): 用户通过自然语言提出需求（例如：“帮我分析一下英伟达”）。 智能识别与数据获取 (NLU \u0026amp; Data Aggregation): 系统精准识别目标公司，并启动数据爬虫和API调用程序，从全网抓取最新的公司财报、新闻、股价等信息。 任务分发至Agent核心 (Task Distribution): 干净、结构化的数据被分发给所有AI Agent。 并行分析 (Parallel Analysis): 传奇投资人Agent (Investor Legends): 从各自的投资哲学出发，进行定性与定量分析。 功能性Agent (Functional Agents): 对估值、风险、技术面、基本面等进行专项量化分析。 整合与生成 (Synthesis \u0026amp; Generation): AI首席投资官 (Chief Investment Strategist): 收集所有Agent的分析报告，主持一场“虚拟投委会”，提炼共识、聚焦分歧，并撰写最终的综合报告。 输出详实报告 (Report Delivery): 一份结构清晰、内容详实的“投资委员会会议纪要”被呈现给用户。 其实主体部分是从ai-hedge-fund那边借鉴的，但是整理成大模型Agent的形式我认为是更加便于操作一些的，以及我们所做的图表和最后生成pdf整理所有Analyses的部分都是为了提高用户体验（更适合中国ai萌新宝宝体质）\n以后有机会的话说不定会好好优化一下，毕竟是自己正儿八经的第一个作品，现在还是一堆bug的状态呢。\n(不过就像上文说的，账号权限被关闭后我连调试资格都没有www\u0026hellip;)\n总结 一个很开心也很有收获的“夏令营”，要有还来！\n希望之后有机会来这里深造，做自己喜欢的东西！\n","date":"2025-07-29T12:34:21+08:00","permalink":"https://Elysium-Seeker.github.io/p/%E5%88%9B%E6%99%BA%E5%BE%AE%E8%AF%BE%E8%90%A5%E8%AE%B0%E5%BD%95ai-stock%E7%AE%80%E4%BB%8B/","title":"创智微课营记录\u0026AI-STOCK简介"},{"content":"原本说好的博客周更的，但这周没有什么特别想做的项目。\n之前的Astrbot想等2.0更新再进行下一步，character.ai的测试没能特别惊艳到我（或许是语言问题），也暂时没有找到别的很吸引我的东西。\n所以先水一篇生活向的，介绍一下自己喜欢的乐队，也算是在朋友圈里宣传一下这些宝藏乐队。（万一能找到同好呢？有同好欢迎交流！！！涉及到这些乐队的话我应该可以短暂摆脱社恐的吧qwq）\n排名不分先后，只是想到的顺序啦。\nVH (Vast \u0026amp; Hazy) 首先就是我最爱的VH！\n来自台湾的创作双人组合，音乐风格“壮阔精致”。（其实我觉得咖咖的声音偏凄美）\n最近听了好多好多，好几次跑步的时候也是放那张live的专辑。\n非常喜欢咖咖空灵的声音和诗意的作词，易祺的编曲和吉他也都很酷炫！\n因为这些歌甚至有点想学吉他了。\n最喜欢的几首歌分别是《与浪之间》，《囚犯》和《轻轻地静静地》。\n其实所有歌都很几天但是这几首尤其听得多，感觉有种自己喜欢的疯感（）\n而且真的超级喜欢《囚犯》的词！！！\n忘了究竟如何开始 越用力越无力\n我早已麻痹 还想如果 可是 也许 说不定\n算了吧\n留不下的 得不到的啊\n才值得一生固执\n——Vast \u0026amp; Hazy 《囚犯》\nDOUDOU（福禄寿） 三胞胎的乐队（DOUDOU只剩两个人了），主唱声线很有辨识度很好听，歌曲也都很有特色，深邃又流行。\n春夏秋冬的live里的歌基本都刷了很多很多遍了。\n《春夏秋冬》，《马》和《我用什么把你留住》更是精选真爱曲目，张口就能唱那种。\n语文太差想不出怎么夸了，但这些歌真的很有感染力很动人。\n《我用什么把你留住》应该是断层的神单了，没听过的快去听！\n因为享受着它的灿烂\n因为忍受着它的腐烂\n你说别追啊 又依依不舍\n所以生命啊 它苦涩如歌\n在这浩瀚星河你是什么\n在她温柔眼眸的你是什么\n闪着光坠落 又依依不舍\n所以生命啊 它璀璨如歌\n——DOUDOU《我用什么把你留住》\n告五人 大概是这篇blog里前二有名的乐队了。\n风格多变且好听，而且感觉很治愈（当然有些歌确实也很致郁啦）\n最喜欢的大概是《爱人错过》吧，《远距离恋爱》也很爱。\n之前大一原计划的电工所春晚表演本来要唱《带你去找夜生活》来着。\n上声乐班的时候老师也说自己声音和男主唱有点像，或许也是喜欢（唱）告五人的原因。\n应该是会一直喜欢下去的乐队呢。\n走过 路过 没遇过\n回头 转头 还是错\n你我不曾感受过 相撞在街口\n相撞在街口\n你妈没有告诉你\n撞到人要说对不起\n本来今天好好的\n爱人就错过\n爱人就错过\n——告五人《爱人错过》\n回春丹 最喜欢的摇滚乐队！以及大概是本文中为数不多非台湾的乐队。\n主唱带磁性的声音很抓耳，学不来但是真的觉得很好听！莫名有种微醺感（\n喜欢的歌有《鲜花》，《艾蜜莉》，还有实验性质很强但很令我着迷的《正义》！\n《跑步时最常听的歌就是《正义》了，第二遍副歌的撕裂感非常能传递力量，以及那种拼尽全力却求而不得的感觉很对我的电波。\n我的心啊我的心\n整栋出租\n处处都给你\n种好的鲜花\n治愈你的白发\n别害怕 别害怕\n有我在的地方\n永远开满了鲜花\n回春丹《鲜花》\n草东没有派对 从小爱到大的摇滚乐队，小学时候就会唱山海了···到现在还是很喜欢。\n可以和回春丹并列我最爱的摇滚了，当然风格不一样，回春丹比草东正面一些。\n但《但》，《山海》和《烂泥》真的都好好听，别的歌像《大风吹》什么的也都很喜欢。\n两张专都是翻来覆去听了好多遍。\n所以草东什么时候发新专。。。\n以及这辈子都不会忘记《山海》怎么唱的！\n我听着那少年的声音\n在还有未来的过去\n渴望着 美好结局\n却没能成为自己\n他明白 他明白 我给不起\n于是转身向山里走去\n他明白 他明白 我给不起\n于是转身向大海走去\n——草东没有派对《山海》\nCrispy脆乐团 台湾的小清新治愈系双人乐队，双主唱，和 VH 有些相似但是更偏治愈一些，男生的声音超级美丽我超爱。\n歌曲都偏民谣风，融合了许多插电，摇滚，流行的元素。\n听感就是很清新很疗愈很温柔，给最近丧丧的自己回了不少血。\n最喜欢的歌是《相爱就是说了一百次对不起》。（这首词也写的很对我喜好）\n相爱就是说了一百次对不起\n长大就是听了一万次没关系\n多少的泪水都没办法\n把我变成你\n如果说这一生有一件事最幸运\n就是赌中亿分之一的机率遇见你\n我还是我自己\n但是没关系\n因为我爱你\n——Crispy脆乐团《相爱就是说了一百次对不起》\n以下是一些因为一两首歌很喜欢的乐队（但能因为一两首歌记住一个乐队说明歌真的很好听！）\n康士坦的变化球 KST，来自台湾的全员主唱摇滚乐队，“渲染力强”，“介于EMO和后摇之间”。（个人觉得是很丧但很美的摇滚）\n对我来说最耳熟能详的曲目是《美好的事可不可以发生在我身上》。\n原唱和各个翻唱版本加起来听了好几十遍吧，已经是张口就来的程度了。\n不管是原唱的撕裂感还是郭采洁翻唱的那种平淡迷茫都很吸引我。\n以及，这首歌的词我也很爱（经典照镜子环节了呢）\n毫无意外的旅程 得到一些可以任性的机会 看似自由啊\n毫无意外的旅程 做到别人也想要做的事 看似精彩啊\n但是事实上发生的从来没想过\n真正想要的都被别人拿走\n没决定太多事就这样到了今天\n然后接下来变成了硬撑的烂局\n再打一剂希望麻醉了痛苦\n只能进 不能退 扛不起 放不下\n不得不走下去\n我们半推半就的人生\n没有和你一样被眷顾的未来\n我们半推半就的人生\n怎么过啊 怎么过啊\n——康士坦的变化球《美好的事可不可以发生在我身上》\n好乐团 又是台湾双人乐队呢，大概往VH方向上靠吧，歌曲很丧但也很温暖。\n（本人最近音乐喜好似乎都是这个方向，可能是现生过的不如意吧qwq）\n喜欢的歌是《我爱你，却不能拯救你》以及《他们说我是没有用的年轻人》\n后者是入坑曲，很喜欢且唱的很熟练，因为真的很high，作词也很疯很酷。\n你会不会和我一样\n觉得自己最多就是这样\n你会不会和我一样\n把希望寄托在别人的身上\n你会不会和我一样\n知道勉强却还在挣扎\n你会不会和我一样\n被生活覆盖梦想和希望\n我们只喜欢小确幸\n放弃去改变不公平\n我们都空有想象力\n你们说的也有道理\n他们说我是没有用的年轻人\n只顾着自己眼中没有其他人\n他们说我是没有用的年轻人\n不懂得牺牲只想过得安稳\n我知道我是没有用的年轻人\n只听见期盼却不曾看到未来\n我知道我是没有用的年轻人\n委屈时只敢这样喃喃自语\n——好乐团《他们说我是没有用的年轻人》\n裁缝铺 很国风很酷的摇滚乐队。（他们粉丝数比我想象的多好多）\n词曲偏中式哲学，主唱声音很显宏大。\n最惊艳我的是《东海老人》，开头非常震撼，属于是最喜欢的几首歌之一了。\n斗笠下的阿bai（伯）说着南音\n没见过的浪漫 使我安然\n也算听过三两言 也算见过是非人\n也算过往南北路\n却总是自以为是 却总是不听他言\n走走走 在哪一站 遇见迷失的自己\n游游游 跳进山海 卸了阴霾的某\n你听见吗 你听到吗 你怕什么\n望着东海 留下秘密 许下愿望\n等我老时 再归来\n——裁缝铺《东海老人》\nOK差不多收工了，还有几个乐队比如万青什么的就不写了，已经很久没听了。（捂脸）\n最后最后最后再重复一遍，收同好交流，孩子想找人聊天！\n","date":"2025-06-14T00:19:21+08:00","permalink":"https://Elysium-Seeker.github.io/p/%E9%9F%B3%E4%B9%90%E5%88%86%E4%BA%AB%E4%B9%90%E9%98%9F%E7%AF%87/","title":"音乐分享——乐队篇"},{"content":"本人是在极其崩溃的情况下开始写这篇博客的。\n初次尝试——DreamSilk丝缕绘梦 一开始没什么点子想起了之前很喜欢的一个“绘画”网站weavesilk。\n于是打算写一个类似的网站，起名叫DreamSilk丝缕绘梦。\n想着是整个和weavesilk差不多的笔刷再加点特效什么的。\n但理想很美好，现实很骨感。\n尝试了很久很久也做不出一个类似weavesilk的笔刷效果。\n而且怎么搞都丑的要死。\n于是被迫决定放弃。\n没有艺术天赋的人真不适合碰美术相关的东西。\n第二个点子——Arealme挑战 心态炸了，决定搞点简单的。\n想起之前经常会刷到Arealme的挑战，决定做个类似的东西。\n比如按序点击50个数什么的。\n这种不涉及审美的东西应该简单吧\nUpdate on 25.6.13\n基本完成了网站的构建。\n自己搭了个测APM的雏形然后尝试（让Copilot大人）增添各种效果以使其满足创新性。\n当每次点击时增加了 “Perfect” ， “Great” 和 “Error” 特效时它就已经离APM测试渐行渐远了。\n于是在一步步修改后他逐渐成了一个类音游竞速小游戏，我甚至给他加上了排行榜。\n但是，还蛮好玩的，起码我是这么觉得的。\n非常期待有人能达成 Perfect 全连，起码我自己做不到呢。\n总结 这次大作业给我的启发还蛮大的，一是AI的能力已经超乎我想象了，后面给网站添加功能的时候，基本上我只要进行大致的文字描述，尊贵的Copilot大人就能帮我美观又简洁的解决问题。哪怕是遇到了一些bug也可以让Copilot解决，他还会顺带讲解bug出现的原因和解决方案。（Copilot会给到改完的代码并提供一键应用功能，真的非常实用！！）\n二是html，js这几个其实也没有\u0026hellip;想象的那么难，其实质和我之前学的C++ or Python也并没有太大差别（其实html更像积木块一点，或者说那个Scratch\u0026hellip;？），起码看懂问题不大，写网页也并没有我想象的那么神秘。（当然之前搭博客的过程对这个肯定是有助益滴，当时看了好多CSS文件，还顺带研究了yaml！）\n但现在还没搞明白怎么部署到真正的网页上，感觉要买服务器，不过Github好像也可以实现，后续试试？\n那就先留个坑啦！\n","date":"2025-06-09T09:00:50+08:00","permalink":"https://Elysium-Seeker.github.io/p/%E8%AE%A1%E7%A7%91%E5%AF%BC%E5%A4%A7%E4%BD%9C%E4%B8%9A%E8%AE%B0%E5%BD%95/","title":"计科导大作业记录"},{"content":"简单记录一下博客搭建过程以及踩过的坑，方便后来者避雷。\n基础搭建 首先主要内容可以先看这个博客，这位老师还在B站录制了视频，照做即可。\n搭建教程\nB站视频\n本博客做出的个性化调整大概就是把亮色模式删了只留暗色模式，其他都没怎么动过。\nStack主题特殊配置 依旧是参考的这位老师的博客\n博客地址\n我这边进行的调整有字体的修改（这个待会说，我用的方法不一样），更新时间的显示，目录折叠\u0026amp;展开，“返回顶部”按钮的添加，以及代码块的折叠\u0026amp;展开。\n在我测试的内容中出问题的主要是把更新时间显示在开头这一操作，注意请修改themes\\hugo-theme-stack-master\\layouts\\partials\\article\\components\\details.html，不要直接在文章中给的文件地址新建details.html并复制粘贴，否则你的文章的各个参数都无法在页面内正常显示。\n背景设置 参考博客\n本博客使用的是其中的点线漂浮背景。\n这边顺便提供一下本人用的 particlesjs-config.json 。（Ctrl + S 直接保存即可）\n字体及字号的调整 （这个问题折磨了笔者一个多小时）\n如果想使用外界字体可以参考上面博客中的方法，也可以参考Stack官方文档的方法。\n我目前使用的就是官方文档中提供的思源宋体。\n然后如果需要调整字号的话可以直接在layouts/partials/head/custom.html中进行修改，这里也提供一下本博客使用的custom.html，我调整了文章的正文字体大小和行间距（即--article-font-size和--article-line-height）\n\u0026lt;style\u0026gt; :root { --article-font-family: \u0026quot;Noto Serif SC\u0026quot;, var(--base-font-family); --article-font-size: 1.7rem; --article-line-height: 1.8; } \u0026lt;/style\u0026gt; \u0026lt;script\u0026gt; (function () { const customFont = document.createElement('link'); customFont.href = \u0026quot;https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;700\u0026amp;display=swap\u0026quot;; customFont.type = \u0026quot;text/css\u0026quot;; customFont.rel = \u0026quot;stylesheet\u0026quot;; document.head.appendChild(customFont); }()); \u0026lt;/script\u0026gt; 但是有个问题，根据官方文档给出的全局CSS变量，我无法在这里修改标题和副标题字号，我自己的解决方法是直接去Stack主题的源代码中进行修改。（其实这里也可以改正文字号等参数）\n在themes\\hugo-theme-stack-master\\assets\\scss\\variables.scss中即可修改正文字体大小及行间距等参数，具体是在如下的部分。注意两个--article-font-size应用场合不同，响应式字体大小（就respond(md)内的）适用于中等及更大的屏幕（比如PC），另一个则适用于移动端。(上面改的地方优先级更高)\n/** * Article content font settings */ :root { --article-font-family: var(--base-font-family); --article-font-size: 1.6rem; @include respond(md) { --article-font-size: 2.0rem; } --article-line-height: 1.85; } 而标题\u0026amp;副标题的相关参数则在themes\\hugo-theme-stack-master\\assets\\scss\\partials\\article.scss中进行修改，具体在如下部分。\n.article-title { font-family: var(--article-font-family); font-weight: 600; margin: 0; color: var(--card-text-color-main); font-size: 2.3rem; @include respond(xl) { font-size: 3rem; } a { color: var(--card-text-color-main); \u0026amp;:hover { color: var(--card-text-color-main); } } } .article-subtitle { font-weight: normal; color: var(--card-text-color-secondary); line-height: 1.5; margin: 0; font-size: 1.9rem; @include respond(xl) { font-size: 2rem; } } 补充说明一个问题，本地调试时对主题源文件的修改似乎不会即使生效，建议每次修改后关闭网站重新进行hugo server -D构建网站。（笔者似乎就是因为这个问题以为起初的修改没能成功，于是浪费了许多时间\u0026hellip;）\n关于博客内容 博客所有的内容都在content文件夹中，包括分类（categories），侧边栏的几个页面（page），以及文章本身（post）。\n在相关的文件夹修改index.zh-cn.md(“关于”和“友链”和“搜索”是直接修改index.md)即可。\n然后给个文章配置的示例吧：\n--- date : 2025-06-06T23:44:50+08:00 draft : false author : \u0026quot;Elysium-Seeker\u0026quot; title : \u0026quot;本博客搭建记录\u0026quot; description : \u0026quot;使用 Hugo + GitHub Page 搭建\u0026quot; image : xxx.jpg categories: - Work --- 一些补充 提供两个好用的网页，对你的配置应该会有帮助：\nMarkdown教程\nStack中文指南（孩子一开始只找到英文的极其痛苦，后来机缘巧合下终于找到中文的了）\n然后 GitHub 时不时会抽风传不上文件，不管有没有挂梯子都会出现这个问题，实在不行换个网络或者换个时间传都可行。\n收工！有问题欢迎和笔者交流！（虽然笔者也不一定会就是啦。）\n","date":"2025-06-06T23:44:50+08:00","permalink":"https://Elysium-Seeker.github.io/p/%E6%9C%AC%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95-%E6%8C%87%E5%8D%97/","title":"本博客搭建记录 \u0026 指南"},{"content":"Part 0.引言 这段时间一直处于一种很迷茫的阶段，感觉自己每天都在毫无意义的事上浪费时间，逐渐走向了无所事事却又一无所成的深渊。眼看着身边的高中同学，那些我曾经认为不如我的人，都一个个取得了令我感到羡慕的成就，而我却原地踏步，一无所成，心里不由得羡慕嫉妒恨，亦不由得反复叩问，我该怎么做，我能做什么？也尝试做了一些努力，也尝试着剖析了自己，可惜一直难有成效，甚至在自我剖析的过程中陷入了自怨自艾，乃至想自暴自弃的境地。\n\u0026emsp;\u0026emsp;直到机缘巧合之下看到这篇《上海交通大学生存手册》，里面的一些文字，尤其立志篇的部分，如迷雾中的灯塔一般，点醒了踟蹰在无边之迷雾中的我。我十分遗憾没能早点看到这篇文章，但或许，如果我没陷入了如今的深渊，未进行了那么多次无果的自省，我也难以从这篇文章中寻得现在明晰的东西。虽说此文并未使我彻底重拾对生活的希望，确立自己今后的志向，为此矢志不渝，但至少，读完了它，我有了前进的方向，明白了我应去何处寻找我的理想，应如何努力去过上逻辑自洽的生活。 Part 1.自陈述 果壳之前 首先，请允许我以一个相对理性客观的自我剖析作为文章的起始，也算是一个前情提要了。我毕业于杭州学军中学教育集团文渊中学信息竞赛班，高考701分浙江省202名在高三下之前甚至连听都没听说过中国科学院大学，父母听了招生宣讲觉得不错于是就在三一的第二位填报了这所学校。然而天有不测风云，曾无比向往SJTU的我三一初试发挥不佳没能通过，而清华的强基计划又由于招生组的政策变动废弃了我的A+。于是，在对这所学校的概念仅有专业任选与尚未明示仅存在于招生老师口中的“本博贯通”，以及所谓双非科研院校的情况下，我懵懵懂懂且孤身一人的来到了这里，抱着从各类校园文里看来的对轻松且丰富多彩的大学生活的憧憬，开始了我到目前为止仍是忙碌且迷茫且毫无色彩的大学生活。\n很遗憾，来到这所学校并没有我想象的轻松，所谓“全员本博”仍停留在纸面上，且这也并不意味着毫无内卷，绩点与排名仍是一个重要的评判标准，而对于之前没能摆脱高考思维的我，亦只把这个作为唯一追求。\n数理基础 于是我就遇到了第一个令我感到矛盾甚至折磨，或者说不能逻辑自洽的问题：\n我学的课，那一大堆的数理基础，对我一个志向在人工智能的人来说，tmd有B用啊？！\n解释一下，国科大有一个十分“特色”的课程设计之处，在大一年级，为了让大家了解各个专业，在做出自己的专业选择前有充分考虑的时间，它并不在大一年级开设专业类课程，而用通用类课程进行代替。本意是好的，然而令我厌恶甚至崩溃的是，所谓的“通识课程”，实际上是别的学校数学物理系的专业课。这些课程难度颇高，还占了大一这一年一大半的学分和绩点，仍怀高考思维的我没有放弃他们的资本，但事实就是我对这些课既没有兴趣，又没有天赋，更明知它们中的大部分在未来，甚至只是在结课以后，就对我们毫无用处。数学倒还好些，尤其是这个物理，我不得不忍着恶心学，但又学不明白，花了最多的时间，却只能及格线以上低分飘过。尤其是在期中考试阶段，为了复习我落下了不少课程进度，又没有能力和兴致补上，于是只能每节课听天书，坐在教室里无所事事的刷知乎或者小红书，内心对自己无比鄙夷，却又不知如何是好。\n社交难题 除了学习，来到大学后的社交我亦有心无力。大一上的我社交圈局限于高中同学，依靠曾经的至交好友解决基本的交流需求，但逐渐地，我发现，每个人都有了自己的生活，似乎只有我还停留在原地。而伴随着一段友情（?）的破裂，一次聊天引发的争吵，我最后发现，能在我需要的时候给予我情感慰藉的朋友，也从我的列表中消失了。有的时候，我甚至只能一个人在高中班级群里刷屏，而数个小时都不会有人回复。坦白说这令人十分痛苦。\n但很有趣的是，当时我的第一反应是剑走偏锋，我搭建了一个qq的bot，想让它角色扮演我的朋友陪我聊天说话，不过很遗憾，这个bot的搭建出了不少岔子，差点把我小号封了，且无论怎么调试，它的语言始终有股明显的ai味，不能与真人比拟。\n其实我并非没有想过真人社交，但是对我来说，我不知道怎么走近别人，也不知道怎么才能让别人走近我。我对陌生人无法信任，甚至充满…恐惧。对我来说，和陌生人进行单独的交流和互动是十分消耗精力的，因为我要努力组织我的措辞，构想我的行为，以期不得罪他人。或许是家庭环境和生长环境的影响，也可能是感情生活的不顺利所导致，我的潜意识里总是有种“宁叫天下人负我，不叫我负天下人”的观念，极端害怕被别人讨厌，给别人带来麻烦，尽管似乎事实上这并不会导致什么。而坦白说，这种性格很难与人产生交互，更别说交朋友了。（摊手）\n而且更令人悲伤的是，由于一开始高考思维导致的对绩点的担忧以及家长的影响，我并没能加入很多有趣的社团，更少了一条社交的途径。总之，在这一切因素的影响下，我并没能在舍友外线下认识超过个位数的同学，至于能称为朋友的更是一个都没有。\nPart 2.读后感 好的自我剖析到此结束，接下来言归正传，回到这本让我深受启发的《上海交通大学生存手册》。\n序 本书的开篇序就令我如醍醐灌顶：\n是啊，过去的我就如文中所言的，沿着一成不变的“典型成功道路”行进着，中考、高考、为了保研追求着GPA，认为上课是唯一必须且不得不做的事，明明有宏大的理想，想去做的事，却被各种琐事牵绊着时间，以至于庸庸碌碌，一事无成。\n欢迎来到上海交通大学 于是我接着往下阅读，来到“欢迎来到上海交通大学”这一篇章，下面的这一段话，又使我明白了许多，重新认真思考了课程的意义。\n平心而论，国科大的数理基础课程教学质量并没有文中描写的那么不堪，老师还是尽力想让同学们听懂的，但亦如文中所言，老师也很难对每个同学负责，统一的讲解也很难保证所有人都听懂，而且据我所知，听懂的，甚至还能坚持听课的（并非出勤）已经是少数人中的少数了。况且，就如我前面自我剖析中所言，这些数理基础，在结课之后，绝大部分就对我们毫无用处了。\n需要的能力与目标 在下一part的部分中，这两块内容也使我深受启发，它使我明白了我应该在本科阶段培养什么能力，可以去做什么，而不是在各类琐事，GPA和迷茫中浪费所有时间。\n失败的思维方式 在此之后，是全手册中真正点醒我的核心部分\n当我看到这一部分时，我终于明白曾经的我困囿于GPA的原因是什么，被琐事占满时间以至于无法追求自己向往的东西的原因是什么，而这样的思维为什么是错误的。\n如果一个人把政策评分（GPA）作为自己的唯一追求，那么他就是这个政策的牺牲品。\n大学四年留给你的是你的人生，在毕业之时，那一串苍白的分数已然作废。\n每天迫于生活压力，毫无主见地忙碌着，可称得上人生一大悲哀。\n真正能称为我们上课理由的，只有我们对科学文化知识的渴望。\n可以说，这些文字犹如一道思维的闪电，照亮了我被黑暗笼罩的精神之海，藉此之光，我看清了困扰我那么久的，生活中逻辑不自洽的核心所在：这样只以GPA为目标的，每天庸庸碌碌忙于琐事一事无成的日子，不是我想要的生活。\n学习的目的与价值 这两段文字也使我感触颇深。诚然，笔者撰写本文时还有着3.90的GPA，15/70的人工智能专业内排名，并不算低。但不得不承认，63分的电磁学期中考成绩，决定了我这学期的GPA注定不会特别耀眼，也从某种意义上，开启了笔者深层次的迷茫。正如上文所言，我迷茫的一大核心矛盾点在于：我努力了，学不会，还知道它没用，那我为它投入那么多时间精力干什么。而曾经囿于高考思维的我只有GPA这一个可供追求的目标，所以不得不强迫自己继续投入时间精力，却又无法不对这些所花的时间的价值表示怀疑。而这两段文字，既让我对GPA不再那么迷信，又第一次真正意义上，明白了学习的目的。\n应该怎样利用时间 这一章的内容给我的启示是如此之多，以至于我决定全文粘贴于此。在读到这篇文章前，我就像文中所说的乖孩子一样，好好上课，好好做作业，虽然作业是ai的，预习复习是没有的，上课是不听的，但坐在教室里也很难干成别的事。于是也就像文中所说的一样，度过了没有作为，甚至没有出息的近一年。\n我为了GPA而坐到教室里，但其一，我没有意识到，GPA的作用其实并没有那么大，其二，我意识到了却没能改变，我只是坐在教室里，却根本没在学习，毫无效率可言。\n我并不是个没有计划的人，但很遗憾，由于各类课程和琐事，我并没能给自己的追求与爱好计划上足够多的时间，也因为各种突如其来的事很难坚持自己的计划。\n所以，我决定翘掉一些我明知听不进去只会浪费时间也没什么作用的课程，转而去做一些更有意义的事，就像文章所言，保证时间不会虚度。\n（插播一个乐子，写到这的时候下午第一节课微积分刚刚开始，之前从来没翘过，但如上文所说，也从来没听过。本来这次想翘的，然后大群里突然说点名，于是不得不迅速到场了，不过只是换个地方码字罢了）\n总有更值得做的事 这个部分我依旧选择全部截取，倒不是说多点醒我，但确实说出了我的心声，这太符合国科大现状了。（对的我说的就是你傻逼数理基础美其名曰提高科学素养实际在以后的研究中一点用都没有）我想，对于我这样的非物理专业学生，相比于毫无用处且学不明白的物理，总有更值得做的事。\n关于研究 也是很有道理的一段话，坦白说我并不知道我对人工智能是否有足够的了解支撑我的兴趣，或许之后可以像文中指导的一样，去读读基础的教科书？（计划+1）\n而看到这些文字，我不得不说，我对未来几乎必然的科研生活产生了…一些恐惧，我自认为不算一个很能够坚持的人，也并不确定自己是否适合科研。但怎么说呢，既然来了国科大，走一步看一步吧。\n至于剩下的部分，有关课程选择技巧和突击备考指南的方法论部分，就不一一详述了。(毕竟是备考指南，要考试了再说qwq)\nPart.3 总结 写不动了，简单结个尾吧。\n总而言之，这篇《上海交通大学生存手册》给我最大的启示就是：要放弃高考思维和被动思维，对GPA祛魅，利用大学时间好好培养自己的各项能力，让生活过的有意义，有价值。\n而于我自身，我目前的想法是：翘掉无用的数理基础课（updated：目前看来翘课有点困难，但带个电脑做自己的事是可行的），做好短期和长期的计划安排，自学感兴趣的东西，通过阅读教科书确定自己对科研有没有兴趣，以及，在感兴趣内容的学习中找到生活的意义，过上逻辑自洽的生活。\n比如坚持运动，学会编曲做音乐，把收藏的哲学课台词课看完。再比如做一个自己的博客，让这篇文章成为我博客里的第一篇文章（或许已经达成了？）；坚持每周看一本书/一部电影，写一篇读后感/观后感…\n希望我能成为理想中的我自己。\n希望我能逻辑自洽的活下去。\n","date":"2025-06-04T08:48:50+08:00","permalink":"https://Elysium-Seeker.github.io/p/%E4%BD%95%E4%BB%A5%E9%80%BB%E8%BE%91%E8%87%AA%E6%B4%BD%E7%9A%84%E7%94%9F%E6%B4%BB/","title":"何以逻辑自洽的生活"}]