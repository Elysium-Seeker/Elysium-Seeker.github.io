<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Work on Elysium-Seeker</title>
        <link>https://Elysium-Seeker.github.io/categories/work/</link>
        <description>Recent content in Work on Elysium-Seeker</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Elysium-Seeker</copyright><atom:link href="https://Elysium-Seeker.github.io/categories/work/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>图像生成论文阅读笔记</title>
        <link>https://Elysium-Seeker.github.io/p/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</link>
        <pubDate>Wed, 03 Dec 2025 09:00:00 +0800</pubDate>
        
        <guid>https://Elysium-Seeker.github.io/p/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</guid>
        <description>&lt;p&gt;被研讨课老师赏识于是派发任务了（？）&lt;/p&gt;
&lt;p&gt;写篇博客记录下阅读心得吧，后面做综述也方便点。&lt;/p&gt;
&lt;p&gt;说是看了不少实际上完全没有阅读全文的能力，都只能对着AI一点点学核心内容。。但应该算大致理解他在干什么了。&lt;/p&gt;
&lt;h2 id=&#34;vae-auto-encoding-variational-bayes-2013&#34;&gt;[VAE] Auto-Encoding Variational Bayes (2013)
&lt;/h2&gt;&lt;p&gt;首先是万物之源 $VAE$。&lt;/p&gt;
&lt;p&gt;$VAE$ ，全称 $Variational\ Autoencoder$ ，中文叫&lt;strong&gt;变分自编码器&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;主要解决的问题是在数据量巨大且潜变量的后验分布复杂不可解的情况下，用正态分布逼近的方法得到一个变分下界，并在对变分下界重参数化后使其可通过标准梯度变化方法求解。&lt;/p&gt;
&lt;p&gt;具体而言，文章搭建了 $Encoder——Decoder$ 的编码/解码器架构双神经网络架构。$Encoder$ 主要负责提取图片特征并得到对应潜变量分布，$Decoder$ 则负责在 $Encoder$ 得到的潜变量分布中采样并转成图片。&lt;/p&gt;
&lt;p&gt;此方法面对的主要问题有：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;潜变量的原始后验分布复杂且不可计算。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$Encoder$ 得到的是潜变量分布的均值和方差，而传统 $Decoder$ 的方式是在对应范围内随机采样，这会导致神经网络反向传播训练更新参数的工作无法进行（没有有意义的梯度）。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;而文章采用的解决办法是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;设计了新的 $Loss\ Function$ : &lt;/p&gt;
$$ Total\ Loss = Reconstrction\ Loss + KL\ Divergence$$&lt;p&gt; $Reconstrction\ Loss$ （ 重构误差 ）依据生成图片与原图的像素差计算，目的是保证两者的相似程度。而 $KL\ Divergence$ （ KL散度 ）则解决了上面的第一个问题，它衡量的是 $Encoder$ 预测的潜变量分布与标准正态分布 $N(0,1)$ 的相似程度。从而控制了潜变量的分布，强制的把它拉到了一个近似的&lt;strong&gt;标准正态分布&lt;/strong&gt;上。&lt;/p&gt;
&lt;p&gt;另外的，数学上证明了： &lt;/p&gt;
$$log\ P(X)=ELBO+KL(后验||近似)$$&lt;p&gt; $log\ P(X)$是数据出现的概率，我们的目的是使其尽量大，而此处的 $KL$ 散度一定是正的，所以只要让 $ELBO$ （ 变分下界，即上面的 $Loss Function$ 的相反数 ）越大就能让 $log\ P(X)$ 尽量大&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;发明了一种新的采样方法：&lt;strong&gt;重参数化&lt;/strong&gt;。通过引入一个独立的，不依赖于任何参数的标准噪音 $\epsilon$ ，并将采样方法从潜变量分布内随机抽样改为采 &lt;/p&gt;
$$z=\mu+\epsilon \times \sigma$$&lt;p&gt;此时 $z$ 由随机值变为带随机常数项的带参确定性算式。且注意到在统计学意义上标准噪音的均值为 $0$，故此时在保证鲁棒性和潜变量分布连贯性的同时使梯度变得有意义，可用于训练时的反向传播。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;于是最后得到了一组可解码图片并将噪音生成对应图片的模型，也是图像生成领域的基石。&lt;/p&gt;
&lt;h2 id=&#34;gan-generative-adversarial-nets-2014&#34;&gt;[GAN] Generative Adversarial Nets (2014)
&lt;/h2&gt;&lt;p&gt;接下来是 $GAN$&lt;/p&gt;
&lt;p&gt;$GAN$，全称 $Generative\ Adversarial\ Nets$ ，&lt;strong&gt;生成对抗网络&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;本质是一种酷炫的思想，相对来讲少一些数学。&lt;/p&gt;
&lt;p&gt;与 $VAE$ 不同，这次我们设计的两个神经网络从相互合作变为了相互对立的关系。&lt;/p&gt;
&lt;p&gt;生成器 $Generator$ （ 后简称 $G$ ）负责用随机的噪音生成图片，而判别器 $Discriminator$ （ 后简称 $D$ ）负责判断给到的图片是真图（ 即来自数据集 ）还是假图（ 即由 $G$ 生成 ）。&lt;/p&gt;
&lt;p&gt;而在训练过程中，依靠梯度反向传播， $G$ 的生图能力和 $D$ 的判别能力将同步提高，在理想情况下最后将达到&lt;strong&gt;纳什均衡&lt;/strong&gt;，即 $G$ 生成的图与真图的分布完全相同，此时 $D$ 只能对所有输入的图都以各二分之一的概率判断是真/假图（否则都有可能降低判断的准确率）且 $D$ 和 $G$ 均不再改变。&lt;/p&gt;
&lt;p&gt;此时的论文中，$G$ 使用的是多层感知机 $MLP$ ( 在后续将被优化成反卷积网络 $CNN$ )，而 $D$ 是个单纯的二分类神经网络。&lt;/p&gt;
&lt;p&gt;尽管 $D$ 只能输出对于图片真假的判断，但是内部参数的梯度（ $Loss$ 对每个权重的偏导数 ）在反向传播的过程中可以通过调整权重矩阵/卷积核使 $G$ 明白假图和真图的差异（ 即怎么改图可以让 $D$ 认为这个图更像真图 ），也使 $D$ 明白怎么更好的判断图是真是假（ 此时一部分卷积核对真图的特征有强烈的正向激活，另一部分卷积核对假图的特征有强烈的负向激活。）&lt;/p&gt;
&lt;p&gt;（其实卷积这一块我还是没完全搞懂，准确来说是神经网络的训练我不是很清晰，后面可以加点补习内容然后更新一下。）&lt;/p&gt;
&lt;p&gt;然后说一下这篇论文最为酷炫的 $minimax$ 公式，就是在训练神经网络中要使其最优的 $Value\ Function $：&lt;/p&gt;
$$\min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]$$&lt;p&gt;对 $D$ 来说，目标是让 $\mathbb{E}&lt;em&gt;{x \sim p&lt;/em&gt;{data}(x)}[\log D(x)] $ 尽量大，让 $ \mathbb{E}_{z \sim p_z(z)}[\log(D(G(z)))]$ 尽量小。也就是对真假的判断尽量准确，而对 $G$ 来说则正相反，二者动态平衡，从而在达到纳什均衡时得到理想的训练效果。&lt;/p&gt;
&lt;p&gt;当然实际上反复横跳的概率远比达到均衡要高。。。所以实际运用上效果没有理想情况那么好，但是这个思路后面发展出的 $DCGAN$ 以及 $StyleGAN$ 系列就能进行很好的工业运用了。&lt;/p&gt;
&lt;h2 id=&#34;dcgan-unsupervised-representation-learning-with-deep-convolutional-generative-adversarial-networks&#34;&gt;[DCGAN] Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks
&lt;/h2&gt;&lt;p&gt;然后是把 $GAN$ 从理论转向工程的 $DCGAN$ 。&lt;/p&gt;
&lt;p&gt;$Deep\ Convolutional\ Generative\ Adversarial\ Networks$，深度卷积生成对抗网络。&lt;/p&gt;
&lt;p&gt;原本的 $GAN$ ，正如上文所受，使用的是 $MLP$ ，效果一般，训练不稳定且出图模糊。&lt;/p&gt;
&lt;p&gt;而 $DCGAN$ 则选择用 $CNN$ 代替 $GAN$，并给出了一套有效可复现可扩展的架构框架，具体而言：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;使用带步长的卷积进行 $CNN$ 的训练（原来是池化的），尽可能保证图片不丢失关键细节且修炼效果更好。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在 $G$ 和 $D$ 的神经网络大部分层中采用批量归一化（即为避免网络层数过深导致的梯度爆炸或消失问题将参数分布强行拉到标准正态分布的级别，并允许模型自身根据需要微调），极大的稳定了训练效果。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;移除网络顶部的全连接层，让整个网络几乎全部由卷积层构成（当然 $D$ 的最后一步判别还得是 $MLP$ ），从而有效增强了模型的“空间感知”能力。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;给 $G$ 和 $D$ 选取了合适的激活函数（神经网络中的必要部分，在每次线性变换后对参数进行非线性扭曲从而提高模型普适性）。在 $G$ 中输出层选取的是 $Tanh$，生成层选取的是 $Relu$ 。而 $D$ 中选择的是 $LeakyRelu$。&lt;/p&gt;
&lt;p&gt;简单说下这几个激活函数是什么：&lt;/p&gt;
&lt;p&gt;（1） $Tanh$ 是双曲正切函数，数学定义是 &lt;/p&gt;
$$Tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$$&lt;p&gt;能把输入值归一化到 $[-1,1]$ 之间，且具有对称性。具体的图像如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://Elysium-Seeker.github.io/p/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/tanh.png&#34;
	width=&#34;633&#34;
	height=&#34;464&#34;
	srcset=&#34;https://Elysium-Seeker.github.io/p/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/tanh_hu_8e5d98f0c4f1df27.png 480w, https://Elysium-Seeker.github.io/p/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/tanh_hu_91754b0fabfc981a.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;$tanh$ 的图像&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;136&#34;
		data-flex-basis=&#34;327px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;（2） $Relu$ 的公式是&lt;/p&gt;
$$f(x)=max(x,0)$$&lt;p&gt;可以有效的缓解梯度消失，是 $CNN$ 的标配，但可能会导致 $Relu\ Dying$ 的问题：当一个神经元一直输出负数的时候，它就再也学不会了。&lt;/p&gt;
&lt;p&gt;(3) $LeakyRelu$ 和 $Relu$ 十分相似，公式为&lt;/p&gt;
$$f(x)=\begin{cases}x\quad if\ x\ge0 \\ \alpha \cdot x\quad if\ x&lt;0\end{cases}$$&lt;p&gt; $\alpha$ 是非常小的常数（ 一定小于1 ）。这种方法有效赋予了梯度在负数区域的流动性，这对需要持续提供梯度以优化 $G$ 和 $D$ 效果的 $D$ 十分必要。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;而在 $GAN$ 得到这些增强变身为 $DCGAN$ 后，文章发现训练后的 $D$ 在无监督特征学习上表现出色，且第一次在视觉上展示了 $z$ 空间（ 潜变量空间 ）的语义。实验得到：&lt;/p&gt;
$$z_{man\ with\ glasses}-z_{man\ without\ glasses}+z_{woman\ without\ glasses}=z_{woman\ with\ glasses}$$&lt;p&gt;这体现了 $GAN$ 的潜变量空间是连续的，结构化的，可被计算，理解与操作的。同时通过可视化分析 $D$ 的卷积核和 $G$ 的记忆，发现 $D$ 在第一层卷积核就学会了检测非常基础的边缘和角落，这证明了 $D$ 确实在学习有意义的底层视觉模式；同时 $G$ 生成的图片往往是训练集中多个样本的平滑混合与重组，这证明了 $G$ 具有泛化能力，它通过解构和重组学会了创造，而不是简单的复制。&lt;/p&gt;
&lt;p&gt;在文章的最后，作者为今后音视频方向的 $GAN$ 发展和潜空间的进一步研究指明了方向，音视频方向暂且按下不表，潜空间则涉及到下一篇论文：&lt;/p&gt;
&lt;h2 id=&#34;stylegan-a-style-based-generator-architecture-for-generative-adversarial-networks&#34;&gt;[StyleGAN] A Style-Based Generator Architecture for Generative Adversarial Networks
&lt;/h2&gt;&lt;p&gt;$StyleGAN$ 重新定义了高质量图像生存的标准，并且至今仍然是许多后续研究的基石（包括待会讲的 $StyleGAN2$ 和 $StyleGAN3$ 。&lt;/p&gt;
&lt;p&gt;原本的 $GAN$ 系列模型虽然已经能应用于生成较清晰的图片，但普遍存在一个巨大的问题：**潜变量空间 $z$ 是纠缠的$。换言之，由于我们把所有数据集拉到了一个标准正态分布里，它们的排列方式和疏密程度很不优雅，从而导致一个参数控制了多个相互独立的视觉特征。&lt;/p&gt;
&lt;p&gt;于是 $StyleGAN$ 重置了原本的 $G$ ，引入了三大核心组件来解决这个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;**映射网络（Mapping Network）：**这是一个独立的 $MLP$ 网络，它不再让原始的噪音向量 $z$ 直接进入生成网络，而是先把它变换成用于解纠缠的潜变量 $w$。这样操作的好处在于我们不再强制地把所有的数据集挤到一个标准正态分布里，而是可以以更适配原来的数据分布的形式表示训练集，从而使得纠缠的 $z$ 变成了更容易线性分离的 $w$。（此后所有工作均在 $w$ 空间上进行，这个空间和真实图像空间相通。）为了让 $w$ 空间更加优美，避免突变， $StyleGAN$ 额外引入了 “感知路径长度” 正则化（ Perceptual Path Length Regularization ，PPL ），使得引入的 $w$ 空间更为平滑，不再有突变。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;自适应实例归一化（AdaIN）：&lt;/strong&gt;&lt;/p&gt;
$$
\text{AdaIN}(x, w) = \sigma(w) \left( \frac{x - \mu(x)}{\sigma(x)} \right) + \beta(w)$$&lt;p&gt; 公式如上，具体可以分为两个步骤，第一步为： &lt;/p&gt;
$$( \frac{x - \mu(x)}{\sigma(x)} )$$&lt;p&gt; 对这张特征图进行实例归一化，保留空间结构信息但是删去风格，第二步是：&lt;/p&gt;
$$\sigma(w) \left( ... \right) + \beta(w)$$&lt;p&gt; 赋予其由当前层 $w$ 指定的全新风格。（这个操作并非只进行一次，而是分层（见下面第三点）进行，每层进行两次 “卷积 + AdaIN” 的操作。）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;分层控制（Hierarchical Control）：生成器从小分辨率逐渐放大到大分辨率，不同分辨率层控制的主要特征不同，且在 $AdaIN$ 的帮助下每层都可以独立控制。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$StyleGAN$ 的核心思想是&lt;strong&gt;分离&lt;/strong&gt;。它把纠缠的潜变量 $z$ 分离成了更纯净的 $w$ ，然后又把 $w$ 蕴含的特征分层注入到生成过程中的每一层，从而实现了对视觉特征的解耦合和精细控制。&lt;/p&gt;
&lt;p&gt;额外的，它固定了生图的起点，原来生成器的起点是随机噪音，而现在是一张固定的低分辨率的图，从而使得潜变量只负责风格，不需负责初始空间布局，进一步加强了模型的解纠缠能力。&lt;/p&gt;
&lt;h2 id=&#34;stylegan2-analyzing-and-improving-the-image-quality-of-stylegan&#34;&gt;[StyleGAN2] Analyzing and Improving the Image Quality of StyleGAN
&lt;/h2&gt;&lt;p&gt;$StyleGAN$ 的升级版，主要在修 Bug 。&lt;/p&gt;
&lt;p&gt;解决的两个主要问题是：&lt;strong&gt;水滴伪影&lt;/strong&gt;和&lt;strong&gt;相位伪影&lt;/strong&gt;。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;首先讲&lt;strong&gt;水滴伪影&lt;/strong&gt;，图示是这样的：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://Elysium-Seeker.github.io/p/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image.png&#34;
	width=&#34;1024&#34;
	height=&#34;1024&#34;
	srcset=&#34;https://Elysium-Seeker.github.io/p/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image_hu_b24140f0025c5b08.png 480w, https://Elysium-Seeker.github.io/p/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image_hu_892824a0b50807f1.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt;&lt;img src=&#34;https://Elysium-Seeker.github.io/p/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-1.png&#34;
	width=&#34;1024&#34;
	height=&#34;1024&#34;
	srcset=&#34;https://Elysium-Seeker.github.io/p/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-1_hu_929a46e249c30895.png 480w, https://Elysium-Seeker.github.io/p/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-1_hu_6f4c407f855d104a.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;可以明显观察到有一颗巨大的深色水滴状伪影在图上。&lt;/p&gt;
&lt;p&gt;它出现的原因是 $StyleGAN$ 最标志性的 &lt;strong&gt;$AdaIN$&lt;/strong&gt; &amp;hellip;&lt;/p&gt;
&lt;p&gt;$AdaIN$ 的第一步是归一化，在抹去风格的同时还破坏了特征图之间的相对强度信息，生成器为了保留某些重要的强信号，在训练中学会了在无关紧要的地方放一个极强的信号，来保留原来的弱小的有用信号们。&lt;/p&gt;
&lt;p&gt;改进方法是放弃归一化，转而直接根据风格（即进来的 $w$ ）调整卷积核。&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;然后是&lt;strong&gt;相位伪影&lt;/strong&gt;：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://Elysium-Seeker.github.io/p/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-4.png&#34;
	width=&#34;1024&#34;
	height=&#34;1024&#34;
	srcset=&#34;https://Elysium-Seeker.github.io/p/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-4_hu_d78a8a13782c9904.png 480w, https://Elysium-Seeker.github.io/p/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-4_hu_d75badeefe52a0b2.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt;&lt;img src=&#34;https://Elysium-Seeker.github.io/p/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-5.png&#34;
	width=&#34;1024&#34;
	height=&#34;1024&#34;
	srcset=&#34;https://Elysium-Seeker.github.io/p/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-5_hu_d185936e7163e389.png 480w, https://Elysium-Seeker.github.io/p/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-5_hu_77e22a970f90e6f9.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt;&lt;img src=&#34;https://Elysium-Seeker.github.io/p/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-6.png&#34;
	width=&#34;1024&#34;
	height=&#34;1024&#34;
	srcset=&#34;https://Elysium-Seeker.github.io/p/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-6_hu_e4c5aafd5823eda2.png 480w, https://Elysium-Seeker.github.io/p/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-6_hu_7e5c5140104ffb3e.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;可以观察到这三张图片里的人物脸转动了，但是牙齿什么的都没动。&lt;/p&gt;
&lt;p&gt;它出现的原因是 $DCGAN$ 中我们采用的分层控制，或者说来自 $ProGAN$ 的 **“ 渐进式增长 ” (Progressive Growing) **。&lt;/p&gt;
&lt;p&gt;这种方法先练低分辨率 （ 4 * 4 ）， 再逐渐往高分辨率训练，每一层都倾向于过早锁定特征。比如低分辨率时模型为了最小化 $Loss Funtion$ 强行把牙齿这种高频细节画出来，在高分辨率的时候就没法改了。&lt;/p&gt;
&lt;p&gt;改进方法是采取新的类似 &lt;strong&gt;ResNet（残差网络）&lt;/strong&gt; 的架构，使用 &lt;strong&gt;Skip Connections（跳跃连接）&lt;/strong&gt;。由每一层练好再练下一层改为输出时把每一层上采样叠加。（换言之，就是原来是一层层训练最后只用最大分辨率的模型，现在是直接每层分开训练不同频特征并在最后把每层图片叠加起来。）&lt;/p&gt;
&lt;p&gt;除了这两个问题之外还有一些别的优化：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;惰性正则化 (Lazy Regularization) :&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这是针对 $StyleGAN$ 里的 PPL 的优化， 作者发现没有必要每一步都算正则化，容易爆显存，每16步算一次就很不错。大大提升了训练效率。&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;路径长度正则化 (Path Length Regularization) 的增强:&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;作者进一步强调了 $PPL$ 的重要性。并提出了能通过 $PPL$ 判断图像好坏。（ $PPL$ 低的图通常质量高）而且 $PPL$ 还让生成器的逆向操作变得十分容易，把真实照片映射回潜变量空间边的非常可靠。（从而成了 P 图神器）&lt;/p&gt;
&lt;h2 id=&#34;stylegan3-alias-free-generative-adversarial-networks&#34;&gt;[StyleGAN3] Alias-Free Generative Adversarial Networks
&lt;/h2&gt;&lt;p&gt;目前为止的最新 $StyleGAN$ 了。（后面全转向 $Diffusion$ 了）&lt;/p&gt;
&lt;p&gt;主要解决了&lt;strong&gt;纹理粘连&lt;/strong&gt;的问题：&lt;/p&gt;
&lt;p&gt;即当尝试用 $StyleGAN2$ 生成动画时，图片中大的部分（如人脸，五官）的变动没有问题，但是皮肤上的毛孔，发丝等细节没有跟着大的部分一起动。&lt;/p&gt;
&lt;p&gt;而 $StyleGAN3$ 能彻底消灭这种粘连，让 $GAN$ 生成的图像具有 &lt;strong&gt;平移/旋转等变性(Equivariance)&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;纹理粘连的原因是整个图像生成体系习以为常的像素网格，和不完美的数学操作。&lt;/p&gt;
&lt;p&gt;在深度学习里，我们默认图像是由一个个离散的像素点组成的矩阵。&lt;/p&gt;
&lt;p&gt;但在物理世界里，光信号是连续的波。&lt;/p&gt;
&lt;p&gt;所以我们的操作会导致&lt;strong&gt;混叠&lt;/strong&gt;现象的出现。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;激活函数（比如前文提到过的 $Relu$ ）或上/下采样操作由于产生了许多数值上的突变，会产生非常多&lt;strong&gt;高频&lt;/strong&gt;的信息。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;而我们的像素网格数是固定的，所以采样数是固定的，根据 &lt;strong&gt;香农采样定理&lt;/strong&gt; ，如果信号变化的频率太快， 超过了像素网格采样频率的二分之一，信号就没法正确记录。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;于是这些高频信号就被伪装成低频信号出现在图像里，产生混叠，从而导致了粘连的发生。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;作者的解决方法是几乎直接推翻了 $StyleGAN2$ 的全部底层逻辑。&lt;/p&gt;
&lt;p&gt;$StyleGAN3$ 的核心思想是：我们要假装神经网络处理的不是“离散的像素”，而是“连续的信号”。&lt;/p&gt;
&lt;p&gt;具体的，有如下三个改动：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;去除“坐标暗示”&lt;/strong&gt; ：神经网络一旦得到关于“绝对坐标”的暗示就会把纹理固定上去，所以作者废除了 $Padding$ （边缘填充），从而使神经网络不知道边缘位置。作者还废除了 $Noise Injection$（随机噪声图）（前面好像忘记讲了，这个是指 $StyleGAN$ 里专门引入了一个随机噪声用来生成头发丝什么的细节，从而使成图更真实，但它是直接加在像素上的，会直接导致纹理固定。）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;换“理想低通滤波器”&lt;/strong&gt;：引入 &lt;strong&gt;Sinc 滤波器&lt;/strong&gt;，在每一层卷积，上采样，激活函数后都过一遍低通滤波器，把高频信号全部切掉。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;重新设计层级结构&lt;/strong&gt; 把所有建立在离散基础上的操作写成了&lt;strong&gt;连续域&lt;/strong&gt;上的数学公式，再近似到离散网络上。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;于是改动后得到一个拥有丝滑的 &lt;strong&gt;“等变性”（Equivariance）&lt;/strong&gt; 的模型，解决了纹理粘连而且可以随便平移，内部世界模型效果更强的模型。&lt;/p&gt;
&lt;p&gt;除此之外，$StyleGAN3$ 还提供了一些数据增强的具体实现方法，比如大量平移旋转训练集什么的，可以更好的做到等变性。&lt;/p&gt;
&lt;h2 id=&#34;singan-singan-learning-a-generative-model-from-a-single-natural-image&#34;&gt;[SinGAN] SinGAN: Learning a Generative Model from a Single Natural Image
&lt;/h2&gt;&lt;p&gt;$ICCV 2019 Best Paper Award$ ，$GAN$ 路子上的邪修。&lt;/p&gt;
&lt;p&gt;主要实现的问题是如何用小样本（比如一张图）生成类似的图片。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://Elysium-Seeker.github.io/p/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-7.png&#34;
	width=&#34;951&#34;
	height=&#34;230&#34;
	srcset=&#34;https://Elysium-Seeker.github.io/p/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-7_hu_17e2f7f3543ac3c4.png 480w, https://Elysium-Seeker.github.io/p/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-7_hu_750233bf9cdfad89.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;413&#34;
		data-flex-basis=&#34;992px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;它只用一张图片作为训练数据，学会这张图片内部的 &lt;strong&gt;“补丁分布”（Internal Patch Distribution）&lt;/strong&gt; ，并借此可生成无线多张相似但布局不同的新图片。&lt;/p&gt;
&lt;p&gt;具体的实现上，它运用了 &lt;strong&gt;“分形”和“局部性”&lt;/strong&gt; 的思想，用一个&lt;strong&gt;11*11&lt;/strong&gt;的小窗口在图片中采样，从而得到海量的训练数据。&lt;/p&gt;
&lt;p&gt;$SinGAN$ 不学“宏观语义”，它学“微观纹理”和“宏观布局”。为了同时学到宏观和微观，它还设计了一个多尺度的分层形式，训练了多个分辨率的 $GAN$ ，从低分辨率开始训练，并以低分辨率的成图上放大后作为输入放到更高分辨率的 $GAN$ 中训练，利用残差学习（类似前面提到的残差网络）逐层添加特征。&lt;/p&gt;
&lt;p&gt;而它的判别器也比较特殊，判别器不看整张图，而是看图中的一个个小方块，判断这些子图是否在原图中出现过，从而保证了生成的图片在细节上真实，而整体布局不与原图完全一样。&lt;/p&gt;
&lt;p&gt;除此之外，为了保证生成的稳定性，作者还额外强制加了一个&lt;strong&gt;重建损失 (Reconstruction Loss)&lt;/strong&gt;，确保在以某组特定输入下模型能生成原图。&lt;/p&gt;
&lt;h2 id=&#34;ddpm-denoising-diffusion-probabilistic-models&#34;&gt;[DDPM] Denoising Diffusion Probabilistic Models
&lt;/h2&gt;&lt;p&gt;终于到 $diffusion$ 了。。&lt;/p&gt;
&lt;p&gt;从这里开始就是生图的另一个路子了。&lt;/p&gt;
&lt;p&gt;$DDPM$ 的核心思想在与，它定义了两个过程，都是 $T$ 步：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;前向过程 (加噪):&lt;/strong&gt; $q(x_t|x_{t-1})$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这是一个&lt;strong&gt;固定的&lt;/strong&gt;过程，没有任何参数需要学习。&lt;/li&gt;
&lt;li&gt;每一次，都在上一时刻的图片上，叠加一点点&lt;strong&gt;高斯噪音&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;到了 $x_{T}$，图片就彻底变成了标准正态分布的噪音 $N(0, I)$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;反向过程 (去噪):&lt;/strong&gt; $p_{\theta}(x_{t-1}|x_t)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这是我们要&lt;strong&gt;训练&lt;/strong&gt;的神经网络。&lt;/li&gt;
&lt;li&gt;它的任务是：&lt;strong&gt;给定一张稍微有点噪的图 $x_t$，预测出它的上一时刻 $x_{t-1}$ 长什么样。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;也就是：&lt;strong&gt;学会“去噪”&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;而通过 $DDPM$ ，我们让模型学会预测我们一开始叠加的高斯噪音。&lt;/p&gt;
&lt;p&gt;这里的 $Loss Function$ 也相当简单，只是一个&lt;strong&gt;均方误差(MSE)&lt;/strong&gt;：&lt;/p&gt;
$$
\text{Loss} = || \epsilon_{\text{真实}} - \epsilon_{\text{预测}}(x_t, t) ||^2$$&lt;p&gt;而我们的训练过程从原来的生图判别演变成了：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;从数据集拿一张真图 $x_0$。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;随机选一个步数 $t$（比如第 500 步）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;随机生成一个噪音 $\epsilon$。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;根据公式，把噪音加到真图上，得到 $x_t$（这一步有闭式解，不用循环 500 次）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;把 $x_t$ 扔给 U-Net 模型，让它猜：“刚才加的那个 $\epsilon$ 是多少？”&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;算 MSE，反向传播。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;接下来补充一些细节：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;训练使用的网络是 $U-Net$ ，因为输入输出都是图片且尺寸相同，$U-Net$ 非常适配。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;当前时间通过 $embedding$ 转化成向量传入网络。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;（论文核心内容）关于一步实现所有前向过程的方法和推导：&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;根据 DDPM 的定义，每一步的前向加噪公式为：
&lt;/p&gt;
$$x_t = \sqrt{\alpha_t} x_{t-1} + \sqrt{1 - \alpha_t} \epsilon_{t-1}$$&lt;p&gt;
其中 $\epsilon_{t-1} \sim \mathcal{N}(0, I)$ 是当步加入的随机高斯噪音。&lt;/p&gt;
&lt;p&gt;我们的目标是推导出 $x_t$ 和 $x_0$ 的直接关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第一步：展开 $x_{t-1}$&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我们知道 $x_{t-1}$ 是由 $x_{t-2}$ 生成的：
&lt;/p&gt;
$$
x_{t-1} = \sqrt{\alpha_{t-1}} x_{t-2} + \sqrt{1 - \alpha_{t-1}} \epsilon_{t-2}
$$&lt;p&gt;将这个式子代入到 $x_t$ 的公式中：&lt;/p&gt;
$$
\begin{aligned}
x_t &amp;= \sqrt{\alpha_t} (\underbrace{\sqrt{\alpha_{t-1}} x_{t-2} + \sqrt{1 - \alpha_{t-1}} \epsilon_{t-2}}_{x_{t-1}}) + \sqrt{1 - \alpha_t} \epsilon_{t-1} \\
&amp;= \sqrt{\alpha_t \alpha_{t-1}} x_{t-2} + \sqrt{\alpha_t (1 - \alpha_{t-1})} \epsilon_{t-2} + \sqrt{1 - \alpha_t} \epsilon_{t-1}
\end{aligned}
$$&lt;p&gt;&lt;strong&gt;第二步：合并噪音 (关键步骤)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;现在式子后面有两项噪音：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\sqrt{\alpha_t (1 - \alpha_{t-1})} \epsilon_{t-2}$&lt;/li&gt;
&lt;li&gt;$\sqrt{1 - \alpha_t} \epsilon_{t-1}$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;根据高斯分布的性质：&lt;strong&gt;两个独立的高斯分布相加，依然是高斯分布，且方差相加。&lt;/strong&gt;
即：若 $X \sim \mathcal{N}(0, \sigma_1^2)$，$Y \sim \mathcal{N}(0, \sigma_2^2)$，则 $X+Y \sim \mathcal{N}(0, \sigma_1^2 + \sigma_2^2)$。&lt;/p&gt;
&lt;p&gt;我们计算这两项噪音合并后的&lt;strong&gt;总方差&lt;/strong&gt;：&lt;/p&gt;
$$\begin{aligned}
\text{总方差} &amp;= (\sqrt{\alpha_t (1 - \alpha_{t-1})})^2 + (\sqrt{1 - \alpha_t})^2 \\
&amp;= \alpha_t (1 - \alpha_{t-1}) + (1 - \alpha_t) \\
&amp;= \alpha_t - \alpha_t \alpha_{t-1} + 1 - \alpha_t \\
&amp;= 1 - \alpha_t \alpha_{t-1}
\end{aligned}$$&lt;p&gt;所以，这两项噪音可以合并为一个新的噪音项：
&lt;/p&gt;
$$\text{合并噪音} = \sqrt{1 - \alpha_t \alpha_{t-1}} \bar{\epsilon} \quad (\text{其中 } \bar{\epsilon} \sim \mathcal{N}(0, I))$$&lt;p&gt;此时公式变为：
&lt;/p&gt;
$$x_t = \sqrt{\alpha_t \alpha_{t-1}} x_{t-2} + \sqrt{1 - \alpha_t \alpha_{t-1}} \bar{\epsilon}$$&lt;p&gt;&lt;strong&gt;第三步：继续展开 (递归)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如果我们继续展开 $x_{t-2}$，以此类推&amp;hellip;&lt;/p&gt;
&lt;p&gt;我们会发现规律：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$x_0$ 前面的系数是所有 $\alpha$ 的乘积的平方根。&lt;/li&gt;
&lt;li&gt;噪音前面的系数始终是 $\sqrt{1 - (\text{所有 } \alpha \text{ 的乘积})}$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;定义累乘系数 $\bar{\alpha}&lt;em&gt;t = \prod&lt;/em&gt;{i=1}^t \alpha_i = \alpha_t \times \alpha_{t-1} \times \dots \times \alpha_1$。&lt;/p&gt;
&lt;p&gt;经过 $t$ 次迭代展开后：&lt;/p&gt;
$$\begin{aligned}
x_t &amp;= \sqrt{\alpha_t \alpha_{t-1} \dots \alpha_1} x_0 + \sqrt{1 - (\alpha_t \alpha_{t-1} \dots \alpha_1)} \epsilon \\
&amp;= \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon
\end{aligned}$$&lt;p&gt;这就是为什么我们不需要写一个 &lt;code&gt;for&lt;/code&gt; 循环迭代 500 次，而是可以直接用闭式解（Closed-form solution）一步算出任意时刻 $t$ 的图像 $x_t$：&lt;/p&gt;
$$x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon$$&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;基于随机微分方程有结论：当加噪步长足够小时，逆过程也可近似于高斯分布，所以反向过程中只要预测上一张图高斯分布的均值和方差。实验发现方差固定即可，只要算均值就行。然后通过类似VAE的思想再进行一些数学操作就可以得到我们需要的 $Loss Function$。且容易由上面的推导发现算出噪声就可以算出原图。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;ddim-denoising-diffusion-implicit-models&#34;&gt;[DDIM] Denoising Diffusion Implicit Models
&lt;/h2&gt;&lt;p&gt;一个对 DDPM 的优化。&lt;/p&gt;
&lt;p&gt;我之所以要花大篇幅讲上面的推导就是因为这里要用。&lt;/p&gt;
&lt;p&gt;DDPM的设计有个限制在于：为了保证逆向过程也是高斯分布，前向过程必须是马尔可夫链（一步一步加噪）。虽然我们在推导后实际上一步解决了前向，但逆向的过程还是得一步步来。&lt;/p&gt;
&lt;p&gt;而正由于我们用的是一步解决的式子 &lt;/p&gt;
$$x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon$$&lt;p&gt; 所以只要 $x_t$ 符合这个分布，Loss就能算，U-Net就能练，中间的加噪过程不重要。&lt;/p&gt;
&lt;p&gt;于是 DDIM 用数学技巧编了个符合边缘分布的过程从而加速了训练，具体如下。&lt;/p&gt;
&lt;p&gt;在 DDPM 的反向采样公式里，有一项是&lt;strong&gt;随机噪音&lt;/strong&gt;：&lt;/p&gt;
$$
x_{t-1} = \text{预测的均值} + \sigma_t \cdot z
$$&lt;p&gt;这个 $z$ 是随机的高斯噪音。正是因为有这个 $z$，每次生成的路径都乱七八糟。&lt;/p&gt;
&lt;p&gt;DDIM 作者推导出了一通用反向公式，其中包含一个可以调节的参数 $\sigma$：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;当 $\sigma = \eta$ (特定值) 时：&lt;/strong&gt; 即&lt;strong&gt;DDPM&lt;/strong&gt; （随机游走）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;当 $\sigma = 0$ 时：&lt;/strong&gt; 随机项消失了，即 &lt;strong&gt;DDIM&lt;/strong&gt; ，此时从 $x_t$ 到 $x_{t-1}$ 的过程变成了&lt;strong&gt;完全确定性 (Deterministic)&lt;/strong&gt; 的。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;虽然数学推导很复杂，但 DDIM 最终落地的采样公式非常直观。它把“计算 $x_{t-1}$”拆解成了三个逻辑步骤：&lt;/p&gt;
&lt;p&gt;假设我们现在在第 $t$ 步，手拿着 $x_t$，U-Net 预测出了噪音 $\epsilon_\theta$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第一步：预测“原本的真图” ($x_0$)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;已知 $x_t$，和大致的噪音 $\epsilon_\theta$，可预测 $x_0$ 为：&lt;/p&gt;
$$
\text{预测的 } x_0 = \frac{x_t - \sqrt{1 - \bar{\alpha}_t}\epsilon_\theta}{\sqrt{\bar{\alpha}_t}}
$$&lt;p&gt;&lt;strong&gt;第二步：指向“下一步的噪音”&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我们要去的下一站是 $x_{t-1}$ （或者跨步到 $x_{t-\Delta}$）。
我们需要计算下一站应该保留多少噪音：&lt;/p&gt;
$$
\text{指向噪音的方向} = \sqrt{1 - \bar{\alpha}_{t-1}} \cdot \epsilon_\theta
$$&lt;p&gt;&lt;strong&gt;第三步：组合 (Vector Addition)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;把“预测的真图”和“指向噪音的方向”按比例拼起来：&lt;/p&gt;
$$
x_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \cdot (\text{预测的 } x_0) + (\text{指向噪音的方向})
$$&lt;p&gt;于是反向过程也可以加速了，且在训练足够完备（预测的噪音够准）的情况下也可以随便加速。&lt;/p&gt;
&lt;p&gt;最后同样的补充一些 DDIM 的特性&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;语义插值 (Semantic Interpolation)&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;因为采样过程是确定的，&lt;strong&gt;一个噪音 $x_T$ 唯一对应一张图 $x_0$&lt;/strong&gt;，所以可以取两个噪音 $z_1$ (生成狗) 和 $z_2$ (生成猫)，做球面线性插值 (Slerp)，中间的噪音生成的图，就是一只“像猫又像狗”的生物，DDPM 做不到这一点，因为随机性太大，插值出来的图通常是崩坏的。。&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;图像重建/逆向 (Inversion)&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这和 StyleGAN 的 Inversion 很像。可以把真图得到一个特定的噪音，并通过这个噪音还原图片。&lt;/p&gt;
&lt;h2 id=&#34;guided-diffusion-diffusion-models-beat-gans-on-image-synthesis&#34;&gt;[Guided Diffusion] Diffusion Models Beat GANs on Image Synthesis
&lt;/h2&gt;&lt;p&gt;到此之前，Diffusion系的模型还是只能做到随机抽卡。（其实 GAN 那边也没有具体讲怎么对应标签，但是可以实现潜空间变量算数的话怎么做标签对应和指定内容生成应该还算显然。）&lt;/p&gt;
&lt;p&gt;那么如何引导去噪过程，让它朝着我们想要的方向去噪生成呢？&lt;/p&gt;
&lt;p&gt;OpenAI 提出了 Classifier Guidance（分类器引导）的方案，具体的：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;准备工作：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;训练一个&lt;strong&gt;图像分类器 (Classifier)&lt;/strong&gt;，比如 ResNet。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;关键点：&lt;/strong&gt; 这个分类器必须在&lt;strong&gt;带噪的图片&lt;/strong&gt;上训练过（能看懂充满雪花点的图）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;生成过程：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;U-Net 说：“我觉得这一步应该往左走 （去噪）。”&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;分类器介入：&lt;/strong&gt; 它看一眼当前的图，计算一下梯度的方向——&lt;strong&gt;“往哪个方向改像素，能让这张图更像‘狗’？”&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;合力：&lt;/strong&gt; 最终的移动方向 = U-Net 的去噪方向 + &lt;strong&gt;分类器的梯度方向&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;数学原理：为什么要加分类器的梯度？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我们的目标是生成符合标签 $y$ 的图像 $x_t$，即从条件概率 $p(x_t|y)$ 中采样。
根据贝叶斯公式 $\log p(x|y) = \log p(x) + \log p(y|x) + C$，两边同时对 $x_t$ 求梯度：&lt;/p&gt;
$$
\underbrace{\nabla_{x_t} \log p(x_t|y)}_{\text{我们需要的目标方向}} = \underbrace{\nabla_{x_t} \log p(x_t)}_{\text{U-Net 的去噪方向}} + \underbrace{\nabla_{x_t} \log p(y|x_t)}_{\text{分类器的梯度方向}}
$$&lt;p&gt;&lt;strong&gt;公式详解：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;$\nabla_{x_t} \log p(x_t)$ (U-Net 的去噪方向)：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这是原始 Diffusion 模型学到的东西。&lt;/li&gt;
&lt;li&gt;它的作用是：&lt;strong&gt;“不管是什么物体，先让这张噪点图变得像一张‘真实的图片’再说。”&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;$\nabla_{x_t} \log p(y|x_t)$ (分类器的梯度方向)：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这是分类器提供的指引。&lt;/li&gt;
&lt;li&gt;它的作用是：&lt;strong&gt;“不管图真不真，先让像素朝着‘更像狗’（标签 $y$）的方向移动。”&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;相加 (合力)：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通常我们会引入一个 &lt;strong&gt;Guidance Scale (引导刻度 $s$)&lt;/strong&gt; 来控制分类器的权重。&lt;/li&gt;
&lt;li&gt;最终公式：&lt;strong&gt;新方向 = 原方向 + $s \times$ 分类器梯度&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这玩意儿的痛点在于分类器训练极其麻烦，而且同时跑两个模型很占显存，于是在谷歌的天才们几个月后提出 CFG 之后这玩意儿基本没什么用了，但它是后续几乎一切的思路基础。&lt;/p&gt;
&lt;h2 id=&#34;cfg-classifier-free-diffusion-guidance&#34;&gt;[CFG] Classifier-Free Diffusion Guidance
&lt;/h2&gt;&lt;p&gt;正如前文所说，CG 的弊端不少：训练麻烦，算力消耗大&amp;hellip;&lt;/p&gt;
&lt;p&gt;于是在此基础上有了 CFG 。&lt;/p&gt;
&lt;p&gt;具体的，它不再需要额外挂载和训练一个专门的分类器，而是在训练时有一定概率丢掉标签，从而让 U-Net 学会了有提示词生成和无提示词生成时的噪音。于是通过如下的采样公式得到最后应有的目标方向：
&lt;/p&gt;
$$
\tilde{\epsilon}_\theta(x_t, c) = \epsilon_\theta(x_t, \emptyset) + w \cdot (\epsilon_\theta(x_t, c) - \epsilon_\theta(x_t, \emptyset))
$$&lt;p&gt;&lt;strong&gt;符号说明：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\tilde{\epsilon}_\theta$：最终用于去噪的预测噪音。&lt;/li&gt;
&lt;li&gt;$\epsilon_\theta(x_t, c)$：&lt;strong&gt;有条件预测&lt;/strong&gt;（Conditional）。&lt;/li&gt;
&lt;li&gt;$\epsilon_\theta(x_t, \emptyset)$：&lt;strong&gt;无条件预测&lt;/strong&gt;（Unconditional）。&lt;/li&gt;
&lt;li&gt;$w$：&lt;strong&gt;引导刻度 (Guidance Scale)&lt;/strong&gt;，控制模型对提示词的遵从程度。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;补充一些 $w$ 的取值影响：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;$w = 1$&lt;/strong&gt;：相当于没有使用 CFG 增强。模型就按标准的条件概率生成。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$w &amp;gt; 1$ (比如 7.0 - 10.0)&lt;/strong&gt;：&lt;strong&gt;这是常用区间。&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;模型会严格遵守提示词。&lt;/li&gt;
&lt;li&gt;图像的语义更清晰，但多样性会降低。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$w$ 过大 (比如 &amp;gt; 20)&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;“过拟合”现象&lt;/strong&gt;：图像会出现颜色过饱和、怪异的伪影、线条崩坏。特征向量被拉得太长，超出了自然图像的流形分布（Manifold）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;stable-diffusion-high-resolution-image-synthesis-with-latent-diffusion-models&#34;&gt;[Stable Diffusion] High-Resolution Image Synthesis with Latent Diffusion Models
&lt;/h2&gt;&lt;p&gt;终于到 Stable Diffusion 了，这是目前的 Diffusion 模型集大成者。&lt;/p&gt;
&lt;p&gt;我们前面讲的以 DDPM 为基础的 Diffusion 模型，计算量十分恐怖，因为它每个像素都要跑，而且训练时还要迭代数百步，每一步都要算梯度和噪音。且最后的成图中把大量的算力浪费在了不重要的细节上，而对语义相关的重点却没有多加关注。&lt;/p&gt;
&lt;p&gt;为了解决这一问题，Stable Diffusion 采取的做法是：去一个压缩空间中进行扩散。&lt;/p&gt;
&lt;p&gt;作者发现图像生成可以拆成两步：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;感知压缩 (Perceptual Compression)&lt;/strong&gt;： 负责处理像素级的细节（清晰度、纹理）。这部分不需要太精细的计算，清晰即可。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;语义生成 (Semantic Generation)&lt;/strong&gt;： 负责处理内容（构图、物体关系）。这部分对算力和理解的需求较高。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;于是 SD 采取了先把图片压缩，让 Diffusion 在小图上工作，最后解压的办法。&lt;/p&gt;
&lt;p&gt;第一步和第三部都是传统 VAE，第一步通过 Encoder 把图片压缩并保留核心语义信息，第三步把抽象的 Latent 转成原图。&lt;/p&gt;
&lt;p&gt;第二步则是 Diffusion ， 但与常规的 Diffusion 不同 ， 我们复原的是 VAE 压缩出的 Latent 。（但具体过程也没什么差别）&lt;/p&gt;
&lt;p&gt;那么在处理好画图问题之后，剩下的就是语义理解问题了。&lt;/p&gt;
&lt;p&gt;为此， SD 引入了 Cross-Attention 机制：&lt;/p&gt;
&lt;p&gt;即语言模型（在原论文中是CLIP，专用的图像对齐模型，现在则是各家的先进LLM）将输入的文本转为一串向量，并将其注入 UNet 。UNet 则通过 Cross-Attention 机制实现在生成时同时接收 Latent 和输入的向量（与 CFG 相似的做法）从而生成指定图片。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;受不了了汇报内容先截止到这里了，感觉完全够讲20分钟了。&lt;/p&gt;
&lt;p&gt;后续要看的内容还有&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Neural ODEs (神经常微分方程)（把 ResNet 的离散变成连续，从而实现材料演化的相关功能）&lt;/li&gt;
&lt;li&gt;NCA (Neural Cellular Automata)（像生物细胞一样无指挥“长”出图片）&lt;/li&gt;
&lt;li&gt;VQGAN (Taming Transformers for High-Resolution Image Synthesis, 2021) （Stable Diffusion 的前置科技。）&lt;/li&gt;
&lt;li&gt;ControlNet（给 Diffusion 加“骨架”。）&lt;/li&gt;
&lt;li&gt;LoRA（微调神器） （快速让模型学会一种新的“材料质感”（比如生锈金属），只需几十张图。）&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>Digital Economy Article Aggregator食用指南</title>
        <link>https://Elysium-Seeker.github.io/p/digital-economy-article-aggregator%E9%A3%9F%E7%94%A8%E6%8C%87%E5%8D%97/</link>
        <pubDate>Wed, 22 Oct 2025 09:00:00 +0800</pubDate>
        
        <guid>https://Elysium-Seeker.github.io/p/digital-economy-article-aggregator%E9%A3%9F%E7%94%A8%E6%8C%87%E5%8D%97/</guid>
        <description>&lt;img src="https://Elysium-Seeker.github.io/p/digital-economy-article-aggregator%E9%A3%9F%E7%94%A8%E6%8C%87%E5%8D%97/image.png" alt="Featured image of post Digital Economy Article Aggregator食用指南" /&gt;&lt;h2 id=&#34;写在前言之前&#34;&gt;写在前言之前
&lt;/h2&gt;&lt;p&gt;哇断更好久好久了。&lt;/p&gt;
&lt;p&gt;果然周更什么的不太可能。&lt;/p&gt;
&lt;p&gt;最近也是忙的没时间写博客了呜呜呜。&lt;/p&gt;
&lt;p&gt;不过也算是攒了很多可以写的素材~&lt;/p&gt;
&lt;p&gt;后面会写一篇Cyberpunk2077的游记（等一回目之后），一篇蓝信封以及志愿者协会目前的故事，然后会把自家的小斯幽调的足够可爱之后写篇文章介绍的。&lt;/p&gt;
&lt;p&gt;那么先来介绍一些这次的产品吧。&lt;/p&gt;
&lt;p&gt;链接：https://github.com/Elysium-Seeker/Digital-Economy-Article-Aggregator&lt;/p&gt;
&lt;h2 id=&#34;前言&#34;&gt;前言
&lt;/h2&gt;&lt;p&gt;这次的灵感来源是数字经济通识课程的日常作业。&lt;/p&gt;
&lt;p&gt;要求我们每周查找相关领域的权威资料并分享。&lt;/p&gt;
&lt;p&gt;而老师特意表扬的一组里除了资料本身还有信源和摘要。&lt;/p&gt;
&lt;p&gt;再加上自己曾经订阅过一个做推上ai信息收集然后每日整理发到邮箱的有趣项目。&lt;/p&gt;
&lt;p&gt;所以就想着自己做一个类似的简单agent了。&lt;/p&gt;
&lt;h2 id=&#34;项目介绍&#34;&gt;项目介绍
&lt;/h2&gt;&lt;p&gt;这是一个数字经济通识课程的资料查找作业工具。&lt;/p&gt;
&lt;p&gt;可用于查找一周内相关领域的权威文章/论文/报告。&lt;/p&gt;
&lt;p&gt;专注于国家级、国际化的数字经济研究，优先提供学术论文和深度行业报告，并确保中英文文献均衡呈现，为您提供全球化的深度洞察。&lt;/p&gt;
&lt;p&gt;可以限定关键词，关键词的英文同义词也将作为检索词，用于查找相关的外文文献。&lt;/p&gt;
&lt;p&gt;还会提供文章的摘要和信息源，帮助用户速览文章的大致内容与确保内容的权威性。&lt;/p&gt;
&lt;h2 id=&#34;开发过程&#34;&gt;开发过程
&lt;/h2&gt;&lt;p&gt;其实一开始没想着弄这个的。。。&lt;/p&gt;
&lt;p&gt;只是研讨课上想用 gemini 查点资料水掉作业而已。&lt;/p&gt;
&lt;p&gt;结果一时起兴点进了 google ai studio 的 build 页面，突然就有了做个 agent 的想法。&lt;/p&gt;
&lt;p&gt;一开始非常努力的在使用英文描述需求。&lt;/p&gt;
&lt;p&gt;其实效果不错，但太费精力了。（尤其是手机打英文真的痛苦）&lt;/p&gt;
&lt;p&gt;而且最终提交上去的摘要肯定还得是中文。&lt;/p&gt;
&lt;p&gt;于是就加了句 turn the website to Chinese。&lt;/p&gt;
&lt;p&gt;结果 build 用的 agent 直接开始用中文回我了。&lt;/p&gt;
&lt;p&gt;那也就用中文一步步改下去了。&lt;/p&gt;
&lt;p&gt;在纯用自然语言描述的情况下能做出这么精致的一个 agent 还是很体现 gemini 2.5 pro 的实力的。&lt;/p&gt;
&lt;p&gt;不过 bug 也确实是要人一步步催着它修的。&lt;/p&gt;
&lt;p&gt;英文改中文后一个冒号的事故改了好几轮 conversation。&lt;/p&gt;
&lt;p&gt;（事故来源是模型内部提取结构化信息的时候中英文冒号混用无法识别导致一篇文章都找不到。）&lt;/p&gt;
&lt;p&gt;以及现在还是时不时会找到无法访问的一些文章，信源也算是良莠不齐（在努力调prompt了但是效果不算特别理想）。&lt;/p&gt;
&lt;p&gt;但总之作为一个作业工具已经非常合格啦！&lt;/p&gt;
&lt;h2 id=&#34;后记&#34;&gt;后记
&lt;/h2&gt;&lt;p&gt;感觉发现了 ai 助力生活的新大陆。&lt;/p&gt;
&lt;p&gt;算是第一次创作一个自己用的到的 agent。&lt;/p&gt;
&lt;p&gt;开森，要再接再厉哦！&lt;/p&gt;
&lt;p&gt;Update：发现其实自己绕了一大圈工作流程，明明找个周更的稳定信源就可以了搞那么麻烦，属于是第一性原理发力了（，项目也已经删了w，期待由相同思路的后续吧。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>创智微课营记录&amp;AI-STOCK简介</title>
        <link>https://Elysium-Seeker.github.io/p/%E5%88%9B%E6%99%BA%E5%BE%AE%E8%AF%BE%E8%90%A5%E8%AE%B0%E5%BD%95ai-stock%E7%AE%80%E4%BB%8B/</link>
        <pubDate>Tue, 29 Jul 2025 12:34:21 +0800</pubDate>
        
        <guid>https://Elysium-Seeker.github.io/p/%E5%88%9B%E6%99%BA%E5%BE%AE%E8%AF%BE%E8%90%A5%E8%AE%B0%E5%BD%95ai-stock%E7%AE%80%E4%BB%8B/</guid>
        <description>&lt;p&gt;&lt;del&gt;周更是不可能周更的，尤其是期末周加社会实践这种阴间配置情况下。暑假也写不动，能更多少更多少吧。&lt;/del&gt;&lt;/p&gt;
&lt;p&gt;前段时间白嫖了上海创智学院的“走进大模型”暑期微课营，并在其中做了个简单的项目，于是写篇文章记录一下。&lt;/p&gt;
&lt;h2 id=&#34;day-1-3&#34;&gt;Day 1-3
&lt;/h2&gt;&lt;p&gt;主要是上课，大模型基础知识什么的。&lt;/p&gt;
&lt;p&gt;对这地方的第一印象是好有钱，免费的营甚至包饭还发衣服（放外面不得四位甚至五位数），而且教室也好大好新。&lt;/p&gt;
&lt;p&gt;还非常幸运的发现有合唱团的学长准备在这里读研，并进行了一番深入的了解。&lt;/p&gt;
&lt;p&gt;第一天下午有这里几个重点项目的介绍，对情境智能很感兴趣。毕竟AI拟人化是我一直以来的目标。&lt;/p&gt;
&lt;p&gt;Actually有点点来这里读研的想法，不过好像很卷很难进&amp;hellip;&lt;/p&gt;
&lt;p&gt;Anyway，有自所保底，也不是很慌，不急（确信）。&lt;/p&gt;
&lt;p&gt;从上课的内容里面算是大致了解到了大模型的部分原理以及微调什么的，有很多是之前知道的甚至高中技术学到的，当然也有很多自己上学期看相关内容了解过的部分，不过还是受益良多的，起码现在算是大致明白了之前一直很好奇的Attention is all you need的大致内容，也了解了模型训练的一个流程什么的。（虽然其实没怎么听，边摸鱼边靠Gemini补课。）&lt;/p&gt;
&lt;p&gt;感觉黑掉模型让它说脏话的部分最有意思了！（bushi）&lt;/p&gt;
&lt;p&gt;（以及上课的时候看旁边华东理工老哥玩进阶20的Slay the Spire 也很有意思。）&lt;/p&gt;
&lt;p&gt;趁着上课时间还摸鱼做了个动态世界模拟器，虽然也是很多bug没调完而且把自己的copilot用到限额了，但是确实很好玩，大模型结合数据库的思路感觉也是后面可以发展的方向。（好像现在Astrbot做的长期记忆就是这个方向搞的，用的milvus数据库）&lt;/p&gt;
&lt;h2 id=&#34;day-4&#34;&gt;Day 4
&lt;/h2&gt;&lt;p&gt;算是最忙的一天了。&lt;/p&gt;
&lt;p&gt;和组里的同济队友合作做出了我们的AI-STOCK。&lt;/p&gt;
&lt;p&gt;虽然现在仍然BUG颇多，而且我暂时也用不了它了（dify临时账号被停了）&lt;/p&gt;
&lt;p&gt;从一开始的模拟人生到后来想到股票方面找到自己之前试用的ai-hedge-fund再到一步步优化调整成我们最后的产品以及画饼PPT和各种完善方向等等，都是非常有意思的过程。&lt;/p&gt;
&lt;p&gt;比预想中的搭建Agent有趣，所学到的知识也远超预期。&lt;/p&gt;
&lt;p&gt;以及上班必备的摸鱼和临时性加班也是让人哭笑不得。&lt;/p&gt;
&lt;p&gt;下午四点就做完&amp;amp;提交了所需要的所有材料然后全组摸了一个小时鱼，结果5点下班的时候突然通知说要做海报要留一会儿才能走，再过了半小时重新询问又发现可以线上做&amp;hellip;&lt;/p&gt;
&lt;p&gt;然后快7点才给到所有材料，又临时给了7点半的ddl。还好有靠谱的美工队友，卡着7点28分最终是发出了材料。&lt;/p&gt;
&lt;p&gt;After all，过的很值得铭记呢。&lt;/p&gt;
&lt;h2 id=&#34;day-5&#34;&gt;Day 5
&lt;/h2&gt;&lt;p&gt;结营日&amp;amp;汇报日，主要流程就是看汇报&amp;amp;做汇报，以及最后的结营。&lt;/p&gt;
&lt;p&gt;很有趣的是发现有两个小组做了和我们相似的内容，不过全部了解过后我坚定的认为我们是做的最完善的。&lt;del&gt;也是画饼画的最好看的。&lt;/del&gt;&lt;/p&gt;
&lt;p&gt;本来不想汇报的本人在队友的推举下不得不一个人搞了全部的汇报，所幸还算顺利没出什么岔子。&lt;/p&gt;
&lt;p&gt;以及从海报上能看出大学生真的都是班味十足，做的都是投资、面试之类的工具类Agent，反观高中生们的作品都是充满想象力的文字类游戏，什么凡人修仙传狼人杀都来了。（虽然相对没有技术含量但确实是大模型擅长的领域而且也确实很好玩呢）&lt;/p&gt;
&lt;p&gt;最后结营的时候又被经费震撼了一下，这边是真有钱免费的营不仅发证书还有纪念品。&lt;/p&gt;
&lt;p&gt;走之前和助教交流了一下这里怎么进，还是很感兴趣的，希望未来能有考到这里，做自己喜欢的项目的机会。&lt;/p&gt;
&lt;h2 id=&#34;ai-stock介绍&#34;&gt;AI-STOCK介绍
&lt;/h2&gt;&lt;p&gt;做都做了肯定要拿来水一水内容的啦！&lt;/p&gt;
&lt;p&gt;把yml文件传到github上了，网址在&lt;a class=&#34;link&#34; href=&#34;https://github.com/Elysium-Seeker/AI-STOCK&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;这里&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;（不过readme是AI写的，画饼什么的都不太可能实现，事实是我现在因为手边没有GPT的API接口甚至无法调试它&amp;hellip;）&lt;/p&gt;
&lt;p&gt;具体来说，这是一个“基于多Agent系统的下一代投资决策支持平台”。&lt;/p&gt;
&lt;p&gt;本质在于让多个大模型从不同的角度给出投资建议并进行整合，以此达到全面和个性化的目的。&lt;/p&gt;
&lt;p&gt;在做投资分析的同时额外还补充了图表生成，信息聚合和新闻等几个模块，勉强算是一站式投资信息门户平台了。&lt;/p&gt;
&lt;p&gt;（其实我觉得很有商业价值啊，做到财经类APP里做个小插件应该很有前途的，就叫&amp;hellip;AI投资展望？）&lt;/p&gt;
&lt;p&gt;关于这个Agent的具体内容大概如下：&lt;/p&gt;
&lt;h3 id=&#34;-核心亮点-key-features&#34;&gt;✨ 核心亮点 (Key Features)
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;🤖 多Agent协同分析 (Multi-Agent Analysis):&lt;/strong&gt; 系统由十多个拥有不同“人格”和分析框架的AI Agent组成，并行工作，确保分析的广度和深度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;🧠 传奇投资人视角 (Legendary Investor Personas):&lt;/strong&gt; 借助沃伦·巴菲特的价值洞察、凯茜·伍德的创新远见、彼得·林奇的成长嗅觉，从经过时间检验的投资哲学中获得智慧。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;📄 综合策略报告 (Comprehensive Reporting):&lt;/strong&gt; 最终输出一份详尽的“投资委员会会议纪要”，清晰呈现每位专家的观点、分歧与共识，以及最终的综合策略建议。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;💬 智能查询与交互 (Intelligent Query &amp;amp; Interaction):&lt;/strong&gt; 不仅能进行深度分析，还能理解用户的日常查询，如“苹果公司最近有什么新闻？”或“给我看一下特斯拉近一年的股价图”。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;🌐 动态数据聚合 (Dynamic Data Aggregation):&lt;/strong&gt; 自动从网页、API等多种渠道获取最新的财务报表、新闻和市场数据，确保分析的时效性。&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;-系统架构与工作流程-architecture--how-it-works&#34;&gt;🏛️ 系统架构与工作流程 (Architecture &amp;amp; How It Works)
&lt;/h3&gt;&lt;p&gt;AI-STOCK 的工作流程旨在模拟一个高效的投研团队从接收任务到提交报告的全过程。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;用户输入 (User Query):&lt;/strong&gt; 用户通过自然语言提出需求（例如：“帮我分析一下英伟达”）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;智能识别与数据获取 (NLU &amp;amp; Data Aggregation):&lt;/strong&gt; 系统精准识别目标公司，并启动数据爬虫和API调用程序，从全网抓取最新的公司财报、新闻、股价等信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;任务分发至Agent核心 (Task Distribution):&lt;/strong&gt; 干净、结构化的数据被分发给所有AI Agent。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;并行分析 (Parallel Analysis):&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;传奇投资人Agent (Investor Legends):&lt;/strong&gt; 从各自的投资哲学出发，进行定性与定量分析。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;功能性Agent (Functional Agents):&lt;/strong&gt; 对估值、风险、技术面、基本面等进行专项量化分析。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;整合与生成 (Synthesis &amp;amp; Generation):&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;AI首席投资官 (Chief Investment Strategist):&lt;/strong&gt; 收集所有Agent的分析报告，主持一场“虚拟投委会”，提炼共识、聚焦分歧，并撰写最终的综合报告。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输出详实报告 (Report Delivery):&lt;/strong&gt; 一份结构清晰、内容详实的“投资委员会会议纪要”被呈现给用户。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;其实主体部分是从ai-hedge-fund那边借鉴的，但是整理成大模型Agent的形式我认为是更加便于操作一些的，以及我们所做的图表和最后生成pdf整理所有Analyses的部分都是为了提高用户体验（&lt;del&gt;更适合中国ai萌新宝宝体质&lt;/del&gt;）&lt;/p&gt;
&lt;p&gt;以后有机会的话说不定会好好优化一下，毕竟是自己正儿八经的第一个作品，现在还是一堆bug的状态呢。&lt;/p&gt;
&lt;p&gt;(不过就像上文说的，账号权限被关闭后我连调试资格都没有www&amp;hellip;)&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结
&lt;/h2&gt;&lt;p&gt;一个很开心也很有收获的“夏令营”，要有还来！&lt;/p&gt;
&lt;p&gt;希望之后有机会来这里深造，做自己喜欢的东西！&lt;/p&gt;
</description>
        </item>
        <item>
        <title>计科导大作业记录</title>
        <link>https://Elysium-Seeker.github.io/p/%E8%AE%A1%E7%A7%91%E5%AF%BC%E5%A4%A7%E4%BD%9C%E4%B8%9A%E8%AE%B0%E5%BD%95/</link>
        <pubDate>Mon, 09 Jun 2025 09:00:50 +0800</pubDate>
        
        <guid>https://Elysium-Seeker.github.io/p/%E8%AE%A1%E7%A7%91%E5%AF%BC%E5%A4%A7%E4%BD%9C%E4%B8%9A%E8%AE%B0%E5%BD%95/</guid>
        <description>&lt;p&gt;本人是在极其崩溃的情况下开始写这篇博客的。&lt;/p&gt;
&lt;h2 id=&#34;初次尝试dreamsilk丝缕绘梦&#34;&gt;初次尝试——DreamSilk丝缕绘梦
&lt;/h2&gt;&lt;p&gt;一开始没什么点子想起了之前很喜欢的一个“绘画”网站&lt;a class=&#34;link&#34; href=&#34;http://weavesilk.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;weavesilk&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;于是打算写一个类似的网站，起名叫DreamSilk丝缕绘梦。&lt;/p&gt;
&lt;p&gt;想着是整个和weavesilk差不多的笔刷再加点特效什么的。&lt;/p&gt;
&lt;p&gt;但理想很美好，现实很骨感。&lt;/p&gt;
&lt;p&gt;尝试了很久很久也做不出一个类似weavesilk的笔刷效果。&lt;/p&gt;
&lt;p&gt;而且怎么搞都丑的要死。&lt;/p&gt;
&lt;p&gt;于是被迫决定放弃。&lt;/p&gt;
&lt;p&gt;没有艺术天赋的人真不适合碰美术相关的东西。&lt;/p&gt;
&lt;h2 id=&#34;第二个点子arealme挑战&#34;&gt;第二个点子——Arealme挑战
&lt;/h2&gt;&lt;p&gt;心态炸了，决定搞点简单的。&lt;/p&gt;
&lt;p&gt;想起之前经常会刷到Arealme的挑战，决定做个类似的东西。&lt;/p&gt;
&lt;p&gt;比如按序点击50个数什么的。&lt;/p&gt;
&lt;p&gt;这种不涉及审美的东西应该简单吧&lt;/p&gt;
&lt;p&gt;Update on 25.6.13&lt;/p&gt;
&lt;p&gt;基本完成了网站的构建。&lt;/p&gt;
&lt;p&gt;自己搭了个测APM的雏形然后尝试（让Copilot大人）增添各种效果以使其满足创新性。&lt;/p&gt;
&lt;p&gt;当每次点击时增加了 “Perfect” ， “Great” 和 “Error” 特效时它就已经离APM测试渐行渐远了。&lt;/p&gt;
&lt;p&gt;于是在一步步修改后他逐渐成了一个类音游竞速小游戏，我甚至给他加上了排行榜。&lt;/p&gt;
&lt;p&gt;但是，还蛮好玩的，起码我是这么觉得的。&lt;/p&gt;
&lt;p&gt;非常期待有人能达成 Perfect 全连，起码我自己做不到呢。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结
&lt;/h2&gt;&lt;p&gt;这次大作业给我的启发还蛮大的，一是AI的能力已经超乎我想象了，后面给网站添加功能的时候，基本上我只要进行大致的文字描述，尊贵的Copilot大人就能帮我美观又简洁的解决问题。哪怕是遇到了一些bug也可以让Copilot解决，他还会顺带讲解bug出现的原因和解决方案。（Copilot会给到改完的代码并提供一键应用功能，真的非常实用！！）&lt;/p&gt;
&lt;p&gt;二是html，js这几个其实也没有&amp;hellip;想象的那么难，其实质和我之前学的C++ or Python也并没有太大差别（其实html更像积木块一点，或者说那个Scratch&amp;hellip;？），起码看懂问题不大，写网页也并没有我想象的那么神秘。（当然之前搭博客的过程对这个肯定是有助益滴，当时看了好多CSS文件，还顺带研究了yaml！）&lt;/p&gt;
&lt;p&gt;但现在还没搞明白怎么部署到真正的网页上，感觉要买服务器，不过Github好像也可以实现，后续试试？&lt;/p&gt;
&lt;p&gt;那就先留个坑啦！&lt;/p&gt;
</description>
        </item>
        <item>
        <title>本博客搭建记录 &amp; 指南</title>
        <link>https://Elysium-Seeker.github.io/p/%E6%9C%AC%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95-%E6%8C%87%E5%8D%97/</link>
        <pubDate>Fri, 06 Jun 2025 23:44:50 +0800</pubDate>
        
        <guid>https://Elysium-Seeker.github.io/p/%E6%9C%AC%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95-%E6%8C%87%E5%8D%97/</guid>
        <description>&lt;p&gt;简单记录一下博客搭建过程以及踩过的坑，方便后来者避雷。&lt;/p&gt;
&lt;h2 id=&#34;基础搭建&#34;&gt;基础搭建
&lt;/h2&gt;&lt;p&gt;首先主要内容可以先看这个博客，这位老师还在B站录制了视频，照做即可。&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://letere-gzj.github.io/hugo-stack/p/hugo/custom-blog/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;搭建教程&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1bovfeaEtQ&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;B站视频&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本博客做出的个性化调整大概就是把亮色模式删了只留暗色模式，其他都没怎么动过。&lt;/p&gt;
&lt;h2 id=&#34;stack主题特殊配置&#34;&gt;Stack主题特殊配置
&lt;/h2&gt;&lt;p&gt;依旧是参考的这位老师的博客&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://letere-gzj.github.io/hugo-stack/p/hugo/custom-stack-theme/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;博客地址&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;我这边进行的调整有字体的修改（这个待会说，我用的方法不一样），更新时间的显示，目录折叠&amp;amp;展开，“返回顶部”按钮的添加，以及代码块的折叠&amp;amp;展开。&lt;/p&gt;
&lt;p&gt;在我测试的内容中出问题的主要是把更新时间显示在开头这一操作，注意请修改&lt;code&gt;themes\hugo-theme-stack-master\layouts\partials\article\components\details.html&lt;/code&gt;，不要直接在文章中给的文件地址新建&lt;code&gt;details.html&lt;/code&gt;并复制粘贴，否则你的文章的各个参数都无法在页面内正常显示。&lt;/p&gt;
&lt;h2 id=&#34;背景设置&#34;&gt;背景设置
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://letere-gzj.github.io/hugo-stack/p/hugo/custom-background/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;参考博客&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本博客使用的是其中的点线漂浮背景。&lt;/p&gt;
&lt;p&gt;这边顺便提供一下本人用的 &lt;a class=&#34;link&#34; href=&#34;particlesjs-config.json&#34; &gt;particlesjs-config.json&lt;/a&gt; 。（Ctrl + S 直接保存即可）&lt;/p&gt;
&lt;h2 id=&#34;字体及字号的调整&#34;&gt;字体及字号的调整
&lt;/h2&gt;&lt;p&gt;（这个问题折磨了笔者一个多小时）&lt;/p&gt;
&lt;p&gt;如果想使用外界字体可以参考上面博客中的方法，也可以参考&lt;a class=&#34;link&#34; href=&#34;https://stack-docs.netlify.app/zh/modify-theme/example-custom-font-family&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Stack官方文档&lt;/a&gt;的方法。&lt;/p&gt;
&lt;p&gt;我目前使用的就是官方文档中提供的思源宋体。&lt;/p&gt;
&lt;p&gt;然后如果需要调整字号的话可以直接在&lt;code&gt;layouts/partials/head/custom.html&lt;/code&gt;中进行修改，这里也提供一下本博客使用的&lt;code&gt;custom.html&lt;/code&gt;，我调整了文章的正文字体大小和行间距（即&lt;code&gt;--article-font-size&lt;/code&gt;和&lt;code&gt;--article-line-height&lt;/code&gt;）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;style&amp;gt;
    :root {
        --article-font-family: &amp;quot;Noto Serif SC&amp;quot;, var(--base-font-family);
        --article-font-size: 1.7rem;
        --article-line-height: 1.8;
    }
&amp;lt;/style&amp;gt;

&amp;lt;script&amp;gt;
        (function () {
            const customFont = document.createElement(&#39;link&#39;);
            customFont.href = &amp;quot;https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;700&amp;amp;display=swap&amp;quot;;
        
            customFont.type = &amp;quot;text/css&amp;quot;;
            customFont.rel = &amp;quot;stylesheet&amp;quot;;
        
            document.head.appendChild(customFont);
        }());
&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;但是有个问题，根据官方文档给出的全局CSS变量，我无法在这里修改标题和副标题字号，我自己的解决方法是直接去Stack主题的源代码中进行修改。（其实这里也可以改正文字号等参数）&lt;/p&gt;
&lt;p&gt;在&lt;code&gt;themes\hugo-theme-stack-master\assets\scss\variables.scss&lt;/code&gt;中即可修改正文字体大小及行间距等参数，具体是在如下的部分。注意两个&lt;code&gt;--article-font-size&lt;/code&gt;应用场合不同，响应式字体大小（就&lt;code&gt;respond(md)&lt;/code&gt;内的）适用于中等及更大的屏幕（比如PC），另一个则适用于移动端。(上面改的地方优先级更高)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;/**
*   Article content font settings
*/
:root {
    --article-font-family: var(--base-font-family);
    --article-font-size: 1.6rem;

    @include respond(md) {
        --article-font-size: 2.0rem;
    }

    --article-line-height: 1.85;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;而标题&amp;amp;副标题的相关参数则在&lt;code&gt;themes\hugo-theme-stack-master\assets\scss\partials\article.scss&lt;/code&gt;中进行修改，具体在如下部分。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;.article-title {
    font-family: var(--article-font-family);
    font-weight: 600;
    margin: 0;
    color: var(--card-text-color-main);
    font-size: 2.3rem;

    @include respond(xl) {
        font-size: 3rem;
    }

    a {
        color: var(--card-text-color-main);

        &amp;amp;:hover {
            color: var(--card-text-color-main);
        }
    }
}

.article-subtitle {
    font-weight: normal;
    color: var(--card-text-color-secondary);
    line-height: 1.5;
    margin: 0;
    font-size: 1.9rem;
    @include respond(xl) {
        font-size: 2rem;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;补充说明一个问题，本地调试时对主题源文件的修改似乎不会即使生效，建议每次修改后关闭网站重新进行&lt;code&gt;hugo server -D&lt;/code&gt;构建网站。（笔者似乎就是因为这个问题以为起初的修改没能成功，于是浪费了许多时间&amp;hellip;）&lt;/p&gt;
&lt;h2 id=&#34;关于博客内容&#34;&gt;关于博客内容
&lt;/h2&gt;&lt;p&gt;博客所有的内容都在&lt;code&gt;content&lt;/code&gt;文件夹中，包括分类（&lt;code&gt;categories&lt;/code&gt;），侧边栏的几个页面（&lt;code&gt;page&lt;/code&gt;），以及文章本身（&lt;code&gt;post&lt;/code&gt;）。&lt;/p&gt;
&lt;p&gt;在相关的文件夹修改&lt;code&gt;index.zh-cn.md&lt;/code&gt;(“关于”和“友链”和“搜索”是直接修改&lt;code&gt;index.md&lt;/code&gt;)即可。&lt;/p&gt;
&lt;p&gt;然后给个文章配置的示例吧：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;---

date : 2025-06-06T23:44:50+08:00
draft : false
author : &amp;quot;Elysium-Seeker&amp;quot;
title : &amp;quot;本博客搭建记录&amp;quot;
description : &amp;quot;使用 Hugo + GitHub Page 搭建&amp;quot;
image : xxx.jpg
categories:
    - Work

---
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;一些补充&#34;&gt;一些补充
&lt;/h2&gt;&lt;p&gt;提供两个好用的网页，对你的配置应该会有帮助：&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://markdown.com.cn/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Markdown教程&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://stack-docs.netlify.app/zh/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Stack中文指南&lt;/a&gt;（孩子一开始只找到英文的极其痛苦，后来机缘巧合下终于找到中文的了）&lt;/p&gt;
&lt;p&gt;然后 GitHub 时不时会抽风传不上文件，不管有没有挂梯子都会出现这个问题，实在不行换个网络或者换个时间传都可行。&lt;/p&gt;
&lt;p&gt;收工！有问题欢迎和笔者交流！（虽然笔者也不一定会就是啦。）&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
